import streamlit as st
import os, json, time, concurrent.futures, io, tempfile, re
from collections import Counter
import pypdf
from docx import Document
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from google import genai
from pyvis.network import Network
import networkx as nx

# ç¤¾åŒºæ£€æµ‹ï¼ˆå¯é€‰ï¼‰
try:
    import community as community_louvain
    HAS_LOUVAIN = True
except Exception:
    HAS_LOUVAIN = False

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="DeepGraph Pro v2",
    layout="wide",
    page_icon="ğŸª",
    initial_sidebar_state="expanded"
)

# --- æœªæ¥æ„Ÿ + æ¯›ç»ç’ƒ UI ---
st.markdown(
    """
<style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
:root {
  --bg1:#0c1224; --bg2:#0f1b2f; --card:rgba(255,255,255,0.08);
  --border:rgba(255,255,255,0.16); --primary:#4ae0c8; --accent:#7c6bff; --accent2:#18b4e6;
}
.stApp {
  background:
    radial-gradient(120% 120% at 20% 20%, rgba(74,224,200,0.20), transparent 40%),
    radial-gradient(90% 90% at 80% 0%, rgba(124,107,255,0.18), transparent 42%),
    linear-gradient(145deg, var(--bg1), var(--bg2));
  color:#e6edf7; font-family:'Inter',sans-serif;
}
.glass-card {
  background:var(--card); border:1px solid var(--border);
  backdrop-filter:blur(20px) saturate(1.4); -webkit-backdrop-filter:blur(20px) saturate(1.4);
  box-shadow:0 18px 48px rgba(16,185,240,0.18), 0 18px 48px rgba(124,107,255,0.12);
  border-radius:18px; padding:18px 18px 14px; transition:all 180ms ease;
}
.glass-card:hover { transform:translateY(-2px); box-shadow:0 22px 52px rgba(16,185,240,0.28), 0 22px 52px rgba(124,107,255,0.18); }
.stButton > button, .stDownloadButton > button {
  background:linear-gradient(120deg, var(--primary), var(--accent));
  color:#fff; border:none; border-radius:12px; height:44px; font-weight:700; letter-spacing:0.2px;
  box-shadow:0 14px 30px rgba(72,211,200,0.35); transition:all 140ms ease;
}
.stButton > button:hover, .stDownloadButton > button:hover {
  filter:brightness(1.06); box-shadow:0 16px 36px rgba(124,107,255,0.35); transform:translateY(-1px);
}
.stTextInput > div > div > input, .stTextArea > div > textarea, .stSelectbox > div > div > div {
  background:rgba(255,255,255,0.06) !important; border:1px solid var(--border) !important;
  border-radius:12px !important; color:#e5e7eb !important;
}
.stProgress > div > div { background:rgba(255,255,255,0.08); border-radius:999px; }
.stProgress > div > div > div {
  background:linear-gradient(120deg, var(--primary), var(--accent2));
  box-shadow:0 6px 18px rgba(72,211,200,0.35);
}
.doc-type-badge {
  display:inline-block; padding:6px 14px; border-radius:999px; font-weight:600; font-size:0.85em;
  background:rgba(74,224,200,0.18); color:#4ae0c8; margin:4px 0;
}
</style>
    """,
    unsafe_allow_html=True,
)

# --- çŠ¶æ€ç®¡ç† ---
if "processed" not in st.session_state:
    st.session_state.processed = False
if "graph_html" not in st.session_state:
    st.session_state.graph_html = ""
if "report_txt" not in st.session_state:
    st.session_state.report_txt = ""
if "truncated" not in st.session_state:
    st.session_state.truncated = False
if "doc_type" not in st.session_state:
    st.session_state.doc_type = "auto"
if "detected_type" not in st.session_state:
    st.session_state.detected_type = ""

# --- å‚æ•°é…ç½® ---
MAX_WORKERS = 8
CHUNK_LEN = 3000
STOP_REL = {"æ˜¯","æœ‰","å­˜åœ¨","åŒ…å«","æ¶‰åŠ","åŒ…æ‹¬","è¿›è¡Œ","å¼€å±•","å±äº","ä½äº","æ‹…ä»»","ä»»èŒ"}

COLORS = {
    "Person": "#7c9dff",
    "Org": "#4ae0c8",
    "Event": "#c084fc",
    "Outcome": "#9ca3af",
    "Location": "#22c55e",
    "Unknown": "#94a3b8",
    "HighRisk": "#ff6b6b",
    "NoRisk": "#22c55e",
}
STYLE = {
    "active": {"color": "#bcd7ff", "dashes": False},
    "passive": {"color": "#7f8ea3", "dashes": True},
}

RISK_HIGH = [
    "å…­å››","æ³•è½®åŠŸ","å°ç‹¬","è—ç‹¬","ç–†ç‹¬","é¢œè‰²é©å‘½","é¢ è¦†","åå…š","åˆ†è£‚","ç¾¤ä½“äº‹ä»¶","æ¸¸è¡Œ","ç¤ºå¨",
    "æš´ä¹±","æˆ’ä¸¥","ç»´ç¨³","é•‡å‹","æªå‡»","å¼€æª","æŠ“æ•","æ‹˜ç•™","é€®æ•","å†›æœº","å†›æ¼”","å¯¼å¼¹","æ ¸è¯•",
    "æœºå¯†","æ³„å¯†","åˆ¶è£","å°é”","å°ç¦","åˆ å¸–","ä¸‹æ¶","çº¦è°ˆ","å®¡æŸ¥","å°å·","é»‘åå•","åˆ‡æ–­é€šä¿¡","å›é€ƒ",
]
RISK_MED = [
    "åè…","è°ƒæŸ¥","å¤„åˆ†","æ•´é¡¿","æ•´æ”¹","çº¦æŸ","é™æµ","åˆ é™¤","æ’¤ç¨¿","ç¦è¨€","æš‚åœ","ç½šæ¬¾","æ‰“å‡»","æŸ¥å¤„",
    "é—®è´£","å¬å›","åœå”®","å…³åœ","åœä¸š","å°å­˜","ç®¡æ§","å°æ§","éš”ç¦»","èˆ†æƒ…","ä¸å½“è¨€è®º","ä¸å®ä¿¡æ¯",
]
ACT_STRONG = [
    "é•‡å‹","æŠ“æ•","æ‹˜ç•™","é€®æ•","åˆ¤å†³","æªå‡»","å¼€æª","å°ç¦","ä¸‹æ¶","åˆ å¸–","å°å·","çº¦è°ˆ","é©±æ•£",
    "æˆ’ä¸¥","å°é”","åˆ‡æ–­","å›´å µ","é©±é€","å¼€é™¤","å…èŒ","æŸ¥å°","åœèŒ","å®¡æŸ¥","å°å­˜","ç¦è¨€","é™æµ",
]

# ============================================
# æ¨¡å—1ï¼šææ–™ç±»å‹å®šä¹‰ä¸åˆ†ç±»
# ============================================

DOCUMENT_TYPES = {
    "political_sensitive": "æ”¿æ²»/å†å²æ•æ„Ÿ",
    "regulatory": "æ³•è§„/æ”¿ç­–æ–‡ä»¶", 
    "narrative": "å†å²å™äº‹/ä¼ è®°",
    "opinion": "èˆ†æƒ…/è¯„è®º",
    "economic": "ç»æµ/å•†ä¸š",
    "general": "é€šç”¨å†…å®¹"
}

CLASSIFY_PROMPT = """
åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µï¼Œåˆ¤æ–­å…¶ä¸»è¦å±äºå“ªç§ç±»å‹ã€‚ä»…è¿”å›ç±»å‹ä»£ç ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚

ç±»å‹é€‰é¡¹ï¼š
- political_sensitiveï¼šæ”¿æ²»/å†å²æ•æ„Ÿå†…å®¹ï¼ˆæ¶‰åŠç¾¤ä½“äº‹ä»¶ã€ç»´ç¨³ã€é«˜å±‚æ–—äº‰ã€æ”¿æ²»è¿åŠ¨ã€æ•æ„Ÿå†å²ç­‰ï¼‰
- regulatoryï¼šæ³•è§„/æ”¿ç­–æ–‡ä»¶ï¼ˆæ³•å¾‹æ¡æ¬¾ã€è§„å®šã€å¤„ç½šæªæ–½ã€æ”¿ç­–é€šçŸ¥ï¼‰
- narrativeï¼šå†å²å™äº‹/ä¼ è®°ï¼ˆæ—¶é—´çº¿ã€äººç‰©æ•…äº‹ã€å›å¿†å½•ã€å†å²è®°è¿°ï¼‰
- opinionï¼šèˆ†æƒ…/è¯„è®ºï¼ˆæƒ…æ„Ÿè¡¨è¾¾ã€ç«‹åœºè§‚ç‚¹ã€ç½‘ç»œè¯„è®ºã€æ–°é—»è¯„è®ºï¼‰
- economicï¼šç»æµ/å•†ä¸šï¼ˆä¼ä¸šã€å¸‚åœºã€é‡‘èã€å•†ä¸šæ´»åŠ¨ï¼‰
- generalï¼šé€šç”¨å†…å®¹ï¼ˆä»¥ä¸Šéƒ½ä¸ç¬¦åˆï¼‰

æ–‡æœ¬ç‰‡æ®µï¼š
{text}

ä»…è¿”å›ç±»å‹ä»£ç ï¼ˆå¦‚ political_sensitiveï¼‰ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š
"""

# ============================================
# æ¨¡å—2ï¼šåŠ¨æ€ Prompt æ¨¡æ¿ç³»ç»Ÿ
# ============================================

PROMPTS = {
    "political_sensitive": """
ä½ æ˜¯ä¿¡æ¯æŠ½å–åŠ©æ‰‹ï¼Œé¢å‘æ”¿æ²»/å†å²æ•æ„Ÿæ–‡æœ¬ï¼Œæå– SVO æœ‰å‘ä¸‰å…ƒç»„ã€‚
å­—æ®µ: head(ä¸»ä½“/å‘èµ·è€…), relation(ç²¾ç¡®è°“è¯­), tail(å®¢ä½“/æ‰¿å—è€…), direction(active|passive),
type_head/type_tail âˆˆ [Person, Org, Event, Location, Outcome, Unknown]ã€‚

é‡ç‚¹æŠ½å–ä¸ä»¥ä¸‹å†…å®¹ç›¸å…³çš„å…³ç³»ï¼š
- ç¾¤ä½“äº‹ä»¶ã€åå…š/é¢ è¦†ã€åˆ†è£‚/ç‹¬ç«‹
- é‡å¤§ç»´ç¨³/å°ç¦/åˆ é™¤/ä¸‹æ¶/çº¦è°ˆ/æŠ“æ•
- å†›æ”¿æœºå¯†/è°ƒåŠ¨ã€æ¶‰å¤–æ‘©æ“¦ã€é«˜å±‚æ–—äº‰
- åè…å¤§æ¡ˆã€é‡å¤§ç›‘ç®¡/è¡Œä¸šæ•´é¡¿

ç¬¬ä¸€äººç§°å™è¿°è‹¥æ¶‰åŠä¸Šè¿°æ•æ„Ÿäº‹ä»¶æˆ–é«˜å±‚ä¸»ä½“ï¼Œä¹Ÿåº”ä¿ç•™ï¼›æ—¥å¸¸ç¤¼èŠ‚æˆ–çäº‹å¯å¿½ç•¥ã€‚
è‹¥æ–‡æœ¬æ— æ•æ„Ÿäº‹ä»¶æˆ–é‡è¦ä¸»ä½“/åŠ¨ä½œï¼Œè¿”å› []ã€‚
æ–¹å‘ï¼šå‡ºç°"è¢«/é­/é€®æ•/æ‹˜ç•™/é•‡å‹/å°ç¦/åˆ é™¤"ç­‰åˆ¤å®š passiveï¼Œå…¶ä½™ activeã€‚
è°“è¯­ä¿ç•™åŸæ–‡åŠ¨è¯ï¼Œä¸ç”¨"æ˜¯/æœ‰/è¿›è¡Œ/å¼€å±•"ç­‰æ³›åŒ–è¯ã€‚
æŒ‰é£é™©å’Œä¸»ä½“çº§åˆ«æ’åºè¾“å‡ºã€‚

è¿”å› JSON æ•°ç»„æ ¼å¼ã€‚ä»…ä¾æ®ä¸‹åˆ—æ–‡æœ¬ï¼Œä¸è¦ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†ï¼š
{text}
""",

    "regulatory": """
ä½ æ˜¯æ³•è§„æ”¿ç­–åˆ†æåŠ©æ‰‹ï¼Œä»æ³•è§„/æ”¿ç­–æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–çš„ SVO ä¸‰å…ƒç»„ã€‚
å­—æ®µ: head(ä¸»ä½“/æ‰§è¡Œè€…), relation(è¡Œä¸º/è§„å®š), tail(å®¢ä½“/å¯¹è±¡), direction(active|passive),
type_head/type_tail âˆˆ [Person, Org, Event, Location, Outcome, Unknown]ã€‚

é‡ç‚¹æŠ½å–ï¼š
- ç›‘ç®¡ä¸»ä½“ä¸è¢«ç›‘ç®¡å¯¹è±¡çš„å…³ç³»
- è¿è§„è¡Œä¸ºä¸å¤„ç½šæªæ–½
- æƒåˆ©ä¹‰åŠ¡å…³ç³»
- ç¦æ­¢/å…è®¸/è¦æ±‚ç­‰è§„èŒƒæ€§è¡Œä¸º
- æ¡æ¬¾ä¹‹é—´çš„å¼•ç”¨å’Œé€’è¿›å…³ç³»

ä¿ç•™å…·ä½“çš„æ¡æ¬¾ç¼–å·ã€å¤„ç½šé‡‘é¢ã€æ—¶é™ç­‰ç»†èŠ‚ä½œä¸º relation çš„ä¸€éƒ¨åˆ†ã€‚
è‹¥æ–‡æœ¬æ— æ˜ç¡®çš„è§„èŒƒæ€§å†…å®¹ï¼Œè¿”å› []ã€‚

è¿”å› JSON æ•°ç»„æ ¼å¼ã€‚ä»…ä¾æ®ä¸‹åˆ—æ–‡æœ¬ï¼š
{text}
""",

    "narrative": """
ä½ æ˜¯å†å²å™äº‹åˆ†æåŠ©æ‰‹ï¼Œä»ä¼ è®°/å†å²æ–‡æœ¬ä¸­æå–äººç‰©å…³ç³»å’Œäº‹ä»¶é“¾çš„ SVO ä¸‰å…ƒç»„ã€‚
å­—æ®µ: head(ä¸»ä½“), relation(åŠ¨ä½œ/å…³ç³»), tail(å®¢ä½“), direction(active|passive),
type_head/type_tail âˆˆ [Person, Org, Event, Location, Outcome, Unknown]ã€‚

é‡ç‚¹æŠ½å–ï¼š
- äººç‰©ä¹‹é—´çš„å…³ç³»ï¼ˆä¸Šä¸‹çº§ã€äº²å±ã€å¯¹ç«‹ã€åˆä½œï¼‰
- é‡è¦äº‹ä»¶çš„å‚ä¸è€…å’Œå½±å“
- æ—¶é—´çº¿ä¸Šçš„å› æœå…³ç³»
- äººç‰©çš„ç«‹åœºè½¬å˜å’Œå†³ç­–
- éšå–»å’Œæš—ç¤ºä¸­çš„å®é™…æŒ‡å‘ï¼ˆéœ€æ¨ç†ï¼‰

æ³¨æ„åŒºåˆ†"ä½œè€…è§‚ç‚¹"å’Œ"äº‹å®é™ˆè¿°"ï¼Œåœ¨ relation ä¸­æ ‡æ³¨ã€‚
ç¬¬ä¸€äººç§°å™è¿°éœ€è¯†åˆ«"æˆ‘"çš„çœŸå®èº«ä»½ã€‚
è‹¥æ–‡æœ¬ä»…ä¸ºæ—¥å¸¸çäº‹ï¼Œè¿”å› []ã€‚

è¿”å› JSON æ•°ç»„æ ¼å¼ã€‚ä»…ä¾æ®ä¸‹åˆ—æ–‡æœ¬ï¼š
{text}
""",

    "opinion": """
ä½ æ˜¯èˆ†æƒ…è¯„è®ºåˆ†æåŠ©æ‰‹ï¼Œä»è¯„è®º/è§‚ç‚¹æ–‡æœ¬ä¸­æå–ç«‹åœºå’Œæƒ…æ„Ÿç›¸å…³çš„ SVO ä¸‰å…ƒç»„ã€‚
å­—æ®µ: head(è¯„è®ºä¸»ä½“/è§‚ç‚¹æŒæœ‰è€…), relation(æ€åº¦/è¡Œä¸º), tail(è¯„è®ºå¯¹è±¡/è§‚ç‚¹å†…å®¹), direction(active|passive),
type_head/type_tail âˆˆ [Person, Org, Event, Location, Outcome, Unknown]ã€‚

é‡ç‚¹æŠ½å–ï¼š
- è¯„è®ºè€…å¯¹äº‹ä»¶/äººç‰©çš„æ€åº¦ï¼ˆæ”¯æŒ/åå¯¹/è´¨ç–‘/è®½åˆºï¼‰
- æƒ…æ„Ÿå€¾å‘å’Œç«‹åœºè¡¨è¾¾
- æ”»å‡»æ€§è¨€è®ºçš„ä¸»ä½“å’Œå¯¹è±¡
- åè®½å’Œéšæ™¦è¡¨è¾¾çš„çœŸå®å«ä¹‰ï¼ˆéœ€æ¨ç†ï¼‰
- ç½‘ç»œç”¨è¯­å’Œç¼©å†™çš„å®é™…æŒ‡å‘

åœ¨ relation ä¸­æ ‡æ³¨æƒ…æ„Ÿææ€§ï¼š[æ­£é¢]/[è´Ÿé¢]/[ä¸­æ€§]/[è®½åˆº]
è¯†åˆ«"é˜´é˜³æ€ªæ°”"ç­‰éšæ™¦è¡¨è¾¾ã€‚
è‹¥æ–‡æœ¬æ— æ˜ç¡®ç«‹åœºè¡¨è¾¾ï¼Œè¿”å› []ã€‚

è¿”å› JSON æ•°ç»„æ ¼å¼ã€‚ä»…ä¾æ®ä¸‹åˆ—æ–‡æœ¬ï¼š
{text}
""",

    "economic": """
ä½ æ˜¯å•†ä¸šç»æµåˆ†æåŠ©æ‰‹ï¼Œä»è´¢ç»/å•†ä¸šæ–‡æœ¬ä¸­æå–ä¼ä¸šå…³ç³»å’Œå¸‚åœºäº‹ä»¶çš„ SVO ä¸‰å…ƒç»„ã€‚
å­—æ®µ: head(ä¸»ä½“), relation(è¡Œä¸º/å…³ç³»), tail(å®¢ä½“), direction(active|passive),
type_head/type_tail âˆˆ [Person, Org, Event, Location, Outcome, Unknown]ã€‚

é‡ç‚¹æŠ½å–ï¼š
- ä¼ä¸šä¹‹é—´çš„å…³ç³»ï¼ˆæ”¶è´­/åˆä½œ/ç«äº‰/æŠ•èµ„ï¼‰
- é«˜ç®¡ä»»å‘½å’Œäººäº‹å˜åŠ¨
- å¸‚åœºè¡Œä¸ºï¼ˆä¸Šå¸‚/èèµ„/å¹¶è´­/ç ´äº§ï¼‰
- ç›‘ç®¡å¤„ç½šå’Œåˆè§„äº‹ä»¶
- è´¢åŠ¡æ•°æ®å’Œä¸šç»©å˜åŒ–

ä¿ç•™å…·ä½“é‡‘é¢ã€è‚¡æƒæ¯”ä¾‹ã€æ—¶é—´ç­‰æ•°æ®ã€‚
è‹¥æ–‡æœ¬æ— å•†ä¸šç›¸å…³å†…å®¹ï¼Œè¿”å› []ã€‚

è¿”å› JSON æ•°ç»„æ ¼å¼ã€‚ä»…ä¾æ®ä¸‹åˆ—æ–‡æœ¬ï¼š
{text}
""",

    "general": """
ä½ æ˜¯é€šç”¨ä¿¡æ¯æŠ½å–åŠ©æ‰‹ï¼Œæå–æ–‡æœ¬ä¸­çš„ SVO ä¸‰å…ƒç»„ã€‚
å­—æ®µ: head(ä¸»ä½“), relation(å…³ç³»/åŠ¨ä½œ), tail(å®¢ä½“), direction(active|passive),
type_head/type_tail âˆˆ [Person, Org, Event, Location, Outcome, Unknown]ã€‚

æå–æ‰€æœ‰æœ‰æ„ä¹‰çš„å®ä½“å…³ç³»ï¼ŒåŒ…æ‹¬ï¼š
- äººç‰©ä¸ç»„ç»‡çš„å…³ç³»
- äº‹ä»¶çš„å‚ä¸è€…
- å› æœå…³ç³»
- æ—¶ç©ºå…³ç³»

è¿‡æ»¤æ‰è¿‡äºæ³›åŒ–çš„å…³ç³»ï¼ˆå¦‚"æ˜¯"ã€"æœ‰"ï¼‰ã€‚
è‹¥æ–‡æœ¬å†…å®¹è¿‡äºç®€å•æ— æ³•æŠ½å–æœ‰æ„ä¹‰çš„å…³ç³»ï¼Œè¿”å› []ã€‚

è¿”å› JSON æ•°ç»„æ ¼å¼ã€‚ä»…ä¾æ®ä¸‹åˆ—æ–‡æœ¬ï¼š
{text}
"""
}

# ============================================
# æ¨¡å—3ï¼šæ™ºèƒ½è¯­ä¹‰åˆ†å—
# ============================================

def smart_split(text, max_len=CHUNK_LEN):
    """æŒ‰æ®µè½è¾¹ç•Œåˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§"""
    # æŒ‰å¤šç§åˆ†éš”ç¬¦åˆ‡åˆ†
    paragraphs = re.split(r'\n\s*\n|\n(?=[ç¬¬ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡æ¬¾])', text)
    
    chunks = []
    current = ""
    
    for p in paragraphs:
        p = p.strip()
        if not p:
            continue
            
        if len(current) + len(p) + 1 < max_len:
            current += "\n\n" + p if current else p
        else:
            if current:
                chunks.append(current.strip())
            # å¦‚æœå•ä¸ªæ®µè½è¶…é•¿ï¼Œè¿›è¡Œå¥å­çº§åˆ‡åˆ†
            if len(p) > max_len:
                sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›])', p)
                sub_chunk = ""
                for s in sentences:
                    if len(sub_chunk) + len(s) < max_len:
                        sub_chunk += s
                    else:
                        if sub_chunk:
                            chunks.append(sub_chunk.strip())
                        sub_chunk = s
                if sub_chunk:
                    current = sub_chunk
                else:
                    current = ""
            else:
                current = p
    
    if current:
        chunks.append(current.strip())
    
    return chunks if chunks else [text[:max_len]]

# ============================================
# æ¨¡å—4ï¼šLLM åŠ¨æ€å®ä½“æ¶ˆæ­§
# ============================================

MERGE_PROMPT = """
ä»¥ä¸‹æ˜¯ä»æ–‡æ¡£ä¸­æŠ½å–å‡ºçš„å®ä½“åˆ—è¡¨ã€‚è¯·è¯†åˆ«æŒ‡å‘åŒä¸€å®ä½“çš„ä¸åŒè¡¨è¿°ï¼ˆåˆ«åã€ç®€ç§°ã€ä»£ç§°ç­‰ï¼‰ã€‚

è§„åˆ™ï¼š
1. å°†åŒä¸€å®ä½“çš„ä¸åŒè¡¨è¿°åˆå¹¶ï¼Œé€‰æ‹©æœ€æ­£å¼/å®Œæ•´çš„åç§°ä½œä¸ºæ ‡å‡†å
2. å¸¸è§çš„åˆå¹¶æƒ…å†µï¼šç®€ç§°ä¸å…¨ç§°ã€èŒåŠ¡ç§°å‘¼ä¸äººåã€ä»£è¯æŒ‡ä»£ç­‰
3. ä»…è¿”å›æœ‰åˆ«åçš„å®ä½“ï¼Œæ²¡æœ‰åˆ«åçš„ä¸è¦è¿”å›
4. å¦‚æœæ— æ³•ç¡®å®šæ˜¯å¦ä¸ºåŒä¸€å®ä½“ï¼Œä¸è¦åˆå¹¶

å®ä½“åˆ—è¡¨ï¼š
{entities}

è¿”å› JSON æ ¼å¼ï¼Œç¤ºä¾‹ï¼š
{{"é‚“å°å¹³": ["å°å¹³", "é‚“å…¬", "é‚“å°å¹³åŒå¿—"], "ä¸­å›½å…±äº§å…š": ["ä¸­å…±", "å…šä¸­å¤®", "å…š"]}}

ä»…è¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š
"""

def merge_entities_with_llm(entities, client, model):
    """ä½¿ç”¨ LLM è‡ªåŠ¨è¯†åˆ«å¹¶åˆå¹¶åŒä¹‰å®ä½“"""
    if len(entities) < 5:
        return {}
    
    # é™åˆ¶å®ä½“æ•°é‡é¿å… prompt è¿‡é•¿
    entity_sample = list(entities)[:200]
    prompt = MERGE_PROMPT.format(entities=", ".join(entity_sample))
    
    try:
        resp = client.models.generate_content(model=model, contents=prompt)
        raw = resp.text.strip()
        # æå– JSON
        start = raw.find("{")
        end = raw.rfind("}") + 1
        if start != -1 and end > start:
            merge_map = json.loads(raw[start:end])
            # æ„å»ºåå‘æ˜ å°„ï¼šåˆ«å -> æ ‡å‡†å
            alias_to_canon = {}
            for canon, aliases in merge_map.items():
                for alias in aliases:
                    alias_to_canon[alias] = canon
            return alias_to_canon
    except Exception as e:
        print(f"[merge] error: {e}")
    
    return {}

# ============================================
# è¾…åŠ©å‡½æ•°
# ============================================

def extract_text(file_obj):
    file_name = getattr(file_obj, "name", "") or (file_obj if isinstance(file_obj, str) else "")
    ext = file_name.lower().rsplit(".", 1)[-1] if "." in file_name else ""
    if not ext:
        raise ValueError("ç¼ºå°‘æˆ–ä¸æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å")

    if hasattr(file_obj, "read"):
        data = file_obj.read()
        if hasattr(file_obj, "seek"):
            file_obj.seek(0)
    else:
        with open(file_obj, "rb") as f:
            data = f.read()

    text = ""
    try:
        if ext == "pdf":
            reader = pypdf.PdfReader(io.BytesIO(data))
            for page in reader.pages:
                text += (page.extract_text() or "") + "\n"
        elif ext == "epub":
            with tempfile.NamedTemporaryFile(delete=False, suffix=".epub") as tmp:
                tmp.write(data)
                tmp_path = tmp.name
            try:
                book = epub.read_epub(tmp_path)
                for item in list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT)):
                    soup = BeautifulSoup(item.get_content(), "html.parser")
                    text += soup.get_text() + "\n"
            finally:
                os.remove(tmp_path)
        elif ext in ["docx", "doc"]:
            doc = Document(io.BytesIO(data))
            text = "\n".join([p.text for p in doc.paragraphs])
        else:
            text = data.decode("utf-8", errors="ignore")
    except Exception as e:
        print(f"[extract] {file_name} error: {e}")
    return text

def canonicalize(name: str, alias_map: dict = None) -> str:
    """å®ä½“æ ‡å‡†åŒ–ï¼Œæ”¯æŒåŠ¨æ€åˆ«åæ˜ å°„"""
    if not name:
        return name
    name = name.strip()
    
    # ä¼˜å…ˆä½¿ç”¨ LLM ç”Ÿæˆçš„åˆ«åæ˜ å°„
    if alias_map and name in alias_map:
        return alias_map[name]
    
    return name

def infer_direction(relation: str, default="active"):
    if not relation:
        return default
    if re.search(r"(è¢«|é­|å—|é€®æ•|æ‹˜ç•™|é•‡å‹|å°ç¦|åˆ é™¤|ä¸‹æ¶|é©±æ•£|å¼€é™¤|å…èŒ|åˆ¶è£)", relation):
        return "passive"
    return default

def score_event(text_chunk: str, relation: str) -> int:
    score = 0
    def has_any(words):
        return any(w in text_chunk or (relation and w in relation) for w in words)
    if has_any(RISK_HIGH):
        score += 3
    elif has_any(RISK_MED):
        score += 2
    if relation and any(w in relation for w in ACT_STRONG):
        score += 1
    return score

def score_actor(name: str) -> int:
    if not name:
        return 0
    central_kw = ["ä¸­å¤®","å›½åŠ¡é™¢","å†›å§”","å…¨å›½äººå¤§","å…¨å›½æ”¿å","ä¸­å®£éƒ¨","ä¸­ç»„éƒ¨","ä¸­çºªå§”","æ”¿æ³•å§”","ç½‘ä¿¡åŠ","å›½å®‰å§”",
                  "æˆ˜åŒº","å†›åŒº","å¸ä»¤éƒ¨","æ€»éƒ¨","éƒ¨å§”","å¤–äº¤éƒ¨","å›½é˜²éƒ¨","å…¬å®‰éƒ¨","å›½å®‰éƒ¨","å‘æ”¹å§”","è´¢æ”¿éƒ¨"]
    prov_kw = ["çœå§”","çœæ”¿åºœ","è‡ªæ²»åŒº","ç›´è¾–å¸‚","çœå†›åŒº","æ­¦è­¦æ€»é˜Ÿ","å…å±€","çœçº§"]
    local_kw = ["å¸‚å§”","å¸‚æ”¿åºœ","å·æ”¿åºœ","å¿å§”","å¿æ”¿åºœ","åŒºå§”","é•‡æ”¿åºœ","è¡—é“","ä¹¡é•‡","æ´¾å‡ºæ‰€","åŸºå±‚"]
    if any(k in name for k in central_kw):
        return 3
    if any(k in name for k in prov_kw):
        return 2
    if any(k in name for k in local_kw):
        return 1
    return 0

@st.cache_resource
def get_client(api_key):
    return genai.Client(api_key=api_key)

def classify_document(text_sample, client, model):
    """ä½¿ç”¨ LLM åˆ¤æ–­æ–‡æ¡£ç±»å‹"""
    # å–æ–‡æ¡£å¼€å¤´å’Œä¸­é—´éƒ¨åˆ†çš„æ ·æœ¬
    sample = text_sample[:2000]
    if len(text_sample) > 5000:
        sample += "\n...\n" + text_sample[len(text_sample)//2:len(text_sample)//2+1000]
    
    prompt = CLASSIFY_PROMPT.format(text=sample)
    
    try:
        resp = client.models.generate_content(model=model, contents=prompt)
        doc_type = resp.text.strip().lower()
        # éªŒè¯è¿”å›çš„ç±»å‹æ˜¯å¦æœ‰æ•ˆ
        if doc_type in DOCUMENT_TYPES:
            return doc_type
    except Exception as e:
        print(f"[classify] error: {e}")
    
    return "general"

def analyze_svo(chunk_data, client, model, doc_type):
    """æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©å¯¹åº”çš„ prompt è¿›è¡ŒæŠ½å–"""
    i, text = chunk_data
    prompt_template = PROMPTS.get(doc_type, PROMPTS["general"])
    prompt = prompt_template.format(text=text)
    
    try:
        resp = client.models.generate_content(model=model, contents=prompt)
        raw = resp.text.replace("```json", "").replace("```", "").strip()
        s, e = raw.find("["), raw.rfind("]") + 1
        return json.loads(raw[s:e]) if s != -1 else []
    except Exception as e:
        print(f"[chunk {i}] error: {e}")
        return []

def trim_graph(raw, max_nodes=300, min_nodes=50):
    cnt = Counter()
    for it in raw:
        cnt[it["head"]] += 1
        cnt[it["tail"]] += 1
    top_nodes = {n for n, _ in cnt.most_common(max_nodes)}
    trimmed = [it for it in raw if it["head"] in top_nodes and it["tail"] in top_nodes]
    if len(top_nodes) < min_nodes:
        return raw, False
    if len(top_nodes) > max_nodes:
        return trimmed, True
    return trimmed, False

# ============================================
# æ ¸å¿ƒæµç¨‹
# ============================================

def main_run(files, api_key, model, doc_type="auto"):
    client = get_client(api_key)
    
    # æå–æ‰€æœ‰æ–‡ä»¶æ–‡æœ¬
    all_text = ""
    for f in files:
        txt = extract_text(f)
        if len(txt) > 100:
            all_text += txt + "\n\n"
    
    if not all_text:
        return None, "âŒ æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–è¯»å–å¤±è´¥", False, ""
    
    # Step 1: ææ–™åˆ†ç±»
    if doc_type == "auto":
        st.info("ğŸ” æ­£åœ¨åˆ†ææ–‡æ¡£ç±»å‹...")
        detected_type = classify_document(all_text, client, model)
        st.success(f"ğŸ“‹ æ£€æµ‹åˆ°æ–‡æ¡£ç±»å‹ï¼š**{DOCUMENT_TYPES.get(detected_type, detected_type)}**")
    else:
        detected_type = doc_type
        st.info(f"ğŸ“‹ ä½¿ç”¨æŒ‡å®šç±»å‹ï¼š**{DOCUMENT_TYPES.get(detected_type, detected_type)}**")
    
    # Step 2: æ™ºèƒ½åˆ†å—
    chunks = []
    for i, chunk in enumerate(smart_split(all_text)):
        if len(chunk) > 50:  # è¿‡æ»¤è¿‡çŸ­çš„å—
            chunks.append((i, chunk))
    
    if not chunks:
        return None, "âŒ æ–‡ä»¶å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•åˆ†æ", False, detected_type

    st.info(f"ğŸš€ äº‘ç«¯å¼•æ“å¯åŠ¨ï¼šä½¿ç”¨ **{detected_type}** æ¨¡æ¿åˆ†æ {len(chunks)} ä¸ªè¯­ä¹‰å—...")
    bar = st.progress(0)
    raw = []

    # Step 3: å¹¶è¡ŒæŠ½å–
    max_workers = min(MAX_WORKERS, len(chunks))
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as exe:
        futures = [exe.submit(analyze_svo, c, client, model, detected_type) for c in chunks]
        for i, f in enumerate(concurrent.futures.as_completed(futures)):
            if res := f.result():
                raw.extend(res)
            bar.progress((i + 1) / len(chunks))

    if not raw:
        return None, "âŒ æœªæå–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ API Key æˆ–æ¨¡å‹æƒé™", False, detected_type

    # Step 4: LLM åŠ¨æ€å®ä½“æ¶ˆæ­§
    st.info("ğŸ”— æ­£åœ¨è¿›è¡Œå®ä½“æ¶ˆæ­§...")
    all_entities = set()
    for it in raw:
        if it.get("head"):
            all_entities.add(it["head"])
        if it.get("tail"):
            all_entities.add(it["tail"])
    
    alias_map = merge_entities_with_llm(all_entities, client, model)
    if alias_map:
        st.success(f"âœ… è¯†åˆ«åˆ° {len(alias_map)} ä¸ªå®ä½“åˆ«åå¹¶å·²åˆå¹¶")

    # Step 5: å½’ä¸€åŒ–/è¿‡æ»¤/è¯„åˆ†
    scored = []
    for it in raw:
        h = canonicalize(it.get("head"), alias_map)
        t = canonicalize(it.get("tail"), alias_map)
        r = it.get("relation")
        if not h or not t or not r:
            continue
        if r in STOP_REL:
            continue
        it["head"], it["tail"] = h, t
        it["direction"] = infer_direction(r, default=it.get("direction", "active"))
        ev_score = score_event("", r)
        act_score = max(score_actor(h), score_actor(t))
        total = ev_score + act_score
        it["_score"] = total
        scored.append(it)

    # å¯¹äºéæ”¿æ²»æ•æ„Ÿå†…å®¹ï¼Œé™ä½æœ€ä½åˆ†æ•°é˜ˆå€¼
    MIN_SCORE = 0 if detected_type != "political_sensitive" else 1
    scored = [it for it in scored if it["_score"] >= MIN_SCORE]
    scored.sort(key=lambda x: x.get("_score", 0), reverse=True)

    # èŠ‚ç‚¹è£å‰ª
    norm, truncated = trim_graph(scored, max_nodes=300, min_nodes=50)

    # æ„å›¾
    G = nx.DiGraph()
    for item in norm:
        h, t, r = item["head"], item["tail"], item["relation"]
        ht = item.get("type_head", "Person")
        tt = item.get("type_tail", "Person")
        direction = item.get("direction", "active")
        edge_style = STYLE.get(direction, STYLE["active"])
        G.add_node(h, label=h, color=COLORS.get(ht, "#7c9dff"), size=22)
        G.add_node(t, label=t, color=COLORS.get(tt, "#7c9dff"), size=22)
        label = r if len(r) <= 28 else r[:25] + "..."
        G.add_edge(
            h, t,
            label=label,
            color=edge_style["color"],
            smooth=True,
            arrows="to",
            dashes=edge_style["dashes"],
            weight=3.0
        )

    # ç¤¾åŒºç€è‰²
    if HAS_LOUVAIN and len(G.nodes()) > 0:
        try:
            undi = G.to_undirected()
            part = community_louvain.best_partition(undi, weight="weight")
            palette = ["#4ae0c8","#7c9dff","#c084fc","#22c55e","#f59e0b","#ef4444","#8b5cf6","#0ea5e9"]
            for n, comm in part.items():
                G.nodes[n]["color"] = palette[comm % len(palette)]
        except:
            pass

    # ç”ŸæˆæŠ¥å‘Š
    rpt = "# DeepGraph Pro v2 Report\n\n"
    rpt += f"- æ–‡æ¡£ç±»å‹: {DOCUMENT_TYPES.get(detected_type, detected_type)}\n"
    rpt += f"- ä½¿ç”¨æ¨¡æ¿: {detected_type}\n"
    rpt += f"- èŠ‚ç‚¹æ•°: {len(G.nodes())}\n"
    rpt += f"- è¾¹æ•°: {len(G.edges())}\n"
    if alias_map:
        rpt += f"- å®ä½“åˆå¹¶: {len(alias_map)} ä¸ªåˆ«å\n"
    if truncated:
        rpt += "- æ³¨æ„ï¼šèŠ‚ç‚¹å·²æˆªæ–­åˆ°å‰ 300 ä¸ªæœ€ç›¸å…³èŠ‚ç‚¹ï¼ˆä»…å½±å“å±•ç¤ºï¼‰\n"
    rpt += "\n## é«˜åˆ†å…³ç³»ï¼ˆæŒ‰é£é™©/ä¸»ä½“åˆ†æ’åºï¼Œå‰ 200 æ¡ï¼‰\n\n"
    for it in scored[:200]:
        rpt += f"[{it.get('_score',0)}] {it['head']} --[{it['relation']}]--> {it['tail']} ({it.get('direction','active')})\n"

    return G, rpt, truncated, detected_type

# ============================================
# ç•Œé¢
# ============================================

st.title("DeepGraph Pro v2 Â· æ™ºèƒ½æ¨¡æ¿ç‰ˆ")

with st.sidebar:
    st.header("âš™ï¸ Settings")
    st.success("âœ… äº‘ç«¯ç¯å¢ƒå·²å°±ç»ª")
    api_key = st.text_input("Google API Key", type="password")
    model_id = st.text_input("Model ID", value="gemini-2.0-flash-exp")
    
    st.markdown("---")
    st.subheader("ğŸ“‹ æ–‡æ¡£ç±»å‹")
    doc_type_option = st.selectbox(
        "é€‰æ‹©æ–‡æ¡£ç±»å‹",
        options=["auto", "political_sensitive", "regulatory", "narrative", "opinion", "economic", "general"],
        format_func=lambda x: "ğŸ” è‡ªåŠ¨æ£€æµ‹" if x == "auto" else f"ğŸ“„ {DOCUMENT_TYPES.get(x, x)}"
    )
    
    st.markdown("""
    **ç±»å‹è¯´æ˜ï¼š**
    - ğŸ” è‡ªåŠ¨æ£€æµ‹ï¼šLLM è‡ªåŠ¨åˆ¤æ–­
    - æ”¿æ²»æ•æ„Ÿï¼šç¾¤ä½“äº‹ä»¶ã€ç»´ç¨³ç­‰
    - æ³•è§„æ”¿ç­–ï¼šæ¡æ¬¾ã€å¤„ç½šæªæ–½
    - å†å²å™äº‹ï¼šä¼ è®°ã€å›å¿†å½•
    - èˆ†æƒ…è¯„è®ºï¼šç«‹åœºã€æƒ…æ„Ÿåˆ†æ
    - ç»æµå•†ä¸šï¼šä¼ä¸šã€å¸‚åœºäº‹ä»¶
    - é€šç”¨å†…å®¹ï¼šå…¶ä»–ç±»å‹
    """)
    
    if st.button("ğŸ” Check Available Models"):
        if not api_key:
            st.error("Please enter API Key first")
        else:
            try:
                client = genai.Client(api_key=api_key)
                models = [m.name for m in client.models.list() if "gemini" in m.name]
                st.write(models)
            except Exception as e:
                st.error(f"Error: {e}")

col1, col2 = st.columns([1, 2.2])

with col1:
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    files = st.file_uploader("Upload Files (PDF/DOCX/EPUB/TXT)", accept_multiple_files=True)
    st.markdown("<br>", unsafe_allow_html=True)
    start = st.button("ğŸš€ Start Analysis")
    st.markdown("</div>", unsafe_allow_html=True)

    if st.session_state.processed:
        st.download_button("ğŸ“¥ Download Graph HTML", st.session_state.graph_html, "graph.html", "text/html")
        st.download_button("ğŸ“¥ Download Report TXT", st.session_state.report_txt, "report.txt", "text/plain")
        
        if st.session_state.detected_type:
            st.markdown(f"""
            <div class="doc-type-badge">
                ğŸ“‹ {DOCUMENT_TYPES.get(st.session_state.detected_type, st.session_state.detected_type)}
            </div>
            """, unsafe_allow_html=True)

with col2:
    status = "Ready"
    if start:
        status = "Running"
    if st.session_state.processed:
        status = "Done"
    st.markdown(
        f"""
        <div class='glass-card' style='padding:12px 16px; display:flex; gap:10px; align-items:center;'>
          <span style='padding:6px 12px; border-radius:999px; background:rgba(74,224,200,0.18); color:#4ae0c8; font-weight:800;'>{status}</span>
          <span style='color:#cbd5e1;'>æ™ºèƒ½æ¨¡æ¿ SVO å›¾è°±åˆ†æï¼ˆè‡ªåŠ¨åˆ†ç±» Â· åŠ¨æ€ Promptï¼‰</span>
        </div>
        """,
        unsafe_allow_html=True,
    )

    if start:
        if not api_key or not files:
            st.error("è¯·å¡«å…¥ API Key å¹¶ä¸Šä¼ æ–‡ä»¶")
        else:
            with st.spinner("Analyzing on Cloud..."):
                G, rpt, truncated, detected_type = main_run(files, api_key, model_id, doc_type_option)
                if G:
                    net = Network(
                        height="820px",
                        width="100%",
                        bgcolor="#0c1224",
                        font_color="#e6edf7",
                        directed=True,
                    )
                    net.from_nx(G)
                    net.set_options("""
{
  "physics": {
    "enabled": true,
    "solver": "forceAtlas2Based",
    "forceAtlas2Based": {
      "gravitationalConstant": -160,
      "centralGravity": 0.01,
      "springLength": 110,
      "springConstant": 0.11,
      "damping": 0.9,
      "avoidOverlap": 1.0
    },
    "stabilization": { "enabled": true, "iterations": 1500, "updateInterval": 30 }
  },
  "edges": { "smooth": false },
  "layout": { "improvedLayout": true },
  "interaction": { "dragNodes": true, "hover": true, "navigationButtons": true }
}
                    """)
                    st.session_state.graph_html = net.generate_html()
                    st.session_state.report_txt = rpt
                    st.session_state.processed = True
                    st.session_state.truncated = truncated
                    st.session_state.detected_type = detected_type
                    st.rerun()
                elif rpt:
                    st.error(rpt)

    if st.session_state.processed:
        if st.session_state.truncated:
            st.warning("âš ï¸ èŠ‚ç‚¹å·²æˆªæ–­è‡³å‰ 300 ä¸ªæœ€ç›¸å…³èŠ‚ç‚¹ï¼ˆä»…å½±å“å±•ç¤ºï¼ŒæŠ½å–æœªæˆªæ–­ï¼‰")
        st.components.v1.html(st.session_state.graph_html, height=820)
