import streamlit as st
import os, json, time, concurrent.futures, io, tempfile, re
from collections import Counter
import pypdf
from docx import Document
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from google import genai
from pyvis.network import Network
import networkx as nx

# ç¤¾åŒºæ£€æµ‹ï¼ˆå¯é€‰ï¼‰
try:
    import community as community_louvain
    HAS_LOUVAIN = True
except Exception:
    HAS_LOUVAIN = False

# --- é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="DeepGraph Pro",
    layout="wide",
    page_icon="ğŸª",
    initial_sidebar_state="expanded"
)

# --- æœªæ¥æ„Ÿ + æ¯›ç»ç’ƒ UI ---
st.markdown(
    """
<style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
:root {
  --bg1:#0c1224; --bg2:#0f1b2f; --card:rgba(255,255,255,0.08);
  --border:rgba(255,255,255,0.16); --primary:#4ae0c8; --accent:#7c6bff; --accent2:#18b4e6;
}
.stApp {
  background:
    radial-gradient(120% 120% at 20% 20%, rgba(74,224,200,0.20), transparent 40%),
    radial-gradient(90% 90% at 80% 0%, rgba(124,107,255,0.18), transparent 42%),
    linear-gradient(145deg, var(--bg1), var(--bg2));
  color:#e6edf7; font-family:'Inter',sans-serif;
}
.glass-card {
  background:var(--card); border:1px solid var(--border);
  backdrop-filter:blur(20px) saturate(1.4); -webkit-backdrop-filter:blur(20px) saturate(1.4);
  box-shadow:0 18px 48px rgba(16,185,240,0.18), 0 18px 48px rgba(124,107,255,0.12);
  border-radius:18px; padding:18px 18px 14px; transition:all 180ms ease;
}
.glass-card:hover { transform:translateY(-2px); box-shadow:0 22px 52px rgba(16,185,240,0.28), 0 22px 52px rgba(124,107,255,0.18); }
.stButton > button, .stDownloadButton > button {
  background:linear-gradient(120deg, var(--primary), var(--accent));
  color:#fff; border:none; border-radius:12px; height:44px; font-weight:700; letter-spacing:0.2px;
  box-shadow:0 14px 30px rgba(72,211,200,0.35); transition:all 140ms ease;
}
.stButton > button:hover, .stDownloadButton > button:hover {
  filter:brightness(1.06); box-shadow:0 16px 36px rgba(124,107,255,0.35); transform:translateY(-1px);
}
.stTextInput > div > div > input, .stTextArea > div > textarea, .stSelectbox > div > div > div {
  background:rgba(255,255,255,0.06) !important; border:1px solid var(--border) !important;
  border-radius:12px !important; color:#e5e7eb !important;
}
.stProgress > div > div { background:rgba(255,255,255,0.08); border-radius:999px; }
.stProgress > div > div > div {
  background:linear-gradient(120deg, var(--primary), var(--accent2));
  box-shadow:0 6px 18px rgba(72,211,200,0.35);
}
</style>
    """,
    unsafe_allow_html=True,
)

# --- çŠ¶æ€ç®¡ç† ---
if "processed" not in st.session_state:
    st.session_state.processed = False
if "graph_html" not in st.session_state:
    st.session_state.graph_html = ""
if "report_txt" not in st.session_state:
    st.session_state.report_txt = ""
if "truncated" not in st.session_state:
    st.session_state.truncated = False

# --- å‚æ•°ï¼ˆé€Ÿåº¦ + ç²¾å‡†ï¼‰ ---
MAX_WORKERS = 8
CHUNK_LEN = 3200
OVERLAP = 200
STOP_REL = {"æ˜¯","æœ‰","å­˜åœ¨","åŒ…å«","æ¶‰åŠ","åŒ…æ‹¬","è¿›è¡Œ","å¼€å±•","å±äº","ä½äº","æ‹…ä»»","ä»»èŒ"}

ALIASES = {
    "é‚“å°å¹³": ["å°å¹³", "é‚“å…¬"],
    "æ¯›æ³½ä¸œ": ["æ¯›ä¸»å¸­", "æ¯›æ³½ä¸œä¸»å¸­"],
    "ä¹ è¿‘å¹³": ["ä¹ ", "è¿‘å¹³"],
    # å¯æ‰©å±•
}

COLORS = {
    "Person": "#7c9dff",
    "Org": "#4ae0c8",
    "Event": "#c084fc",
    "Outcome": "#9ca3af",
    "Location": "#22c55e",
    "Unknown": "#94a3b8",
    "HighRisk": "#ff6b6b",
    "NoRisk": "#22c55e",
}
STYLE = {
    "active": {"color": "#bcd7ff", "dashes": False},
    "passive": {"color": "#7f8ea3", "dashes": True},
}

RISK_HIGH = [
    "å…­å››","æ³•è½®åŠŸ","å°ç‹¬","è—ç‹¬","ç–†ç‹¬","é¢œè‰²é©å‘½","é¢ è¦†","åå…š","åˆ†è£‚","ç¾¤ä½“äº‹ä»¶","æ¸¸è¡Œ","ç¤ºå¨",
    "æš´ä¹±","æˆ’ä¸¥","ç»´ç¨³","é•‡å‹","æªå‡»","å¼€æª","æŠ“æ•","æ‹˜ç•™","é€®æ•","å†›æœº","å†›æ¼”","å¯¼å¼¹","æ ¸è¯•",
    "æœºå¯†","æ³„å¯†","åˆ¶è£","å°é”","å°ç¦","åˆ å¸–","ä¸‹æ¶","çº¦è°ˆ","å®¡æŸ¥","å°å·","é»‘åå•","åˆ‡æ–­é€šä¿¡","å›é€ƒ",
]
RISK_MED = [
    "åè…","è°ƒæŸ¥","å¤„åˆ†","æ•´é¡¿","æ•´æ”¹","çº¦æŸ","é™æµ","åˆ é™¤","æ’¤ç¨¿","ç¦è¨€","æš‚åœ","ç½šæ¬¾","æ‰“å‡»","æŸ¥å¤„",
    "é—®è´£","å¬å›","åœå”®","å…³åœ","åœä¸š","å°å­˜","ç®¡æ§","å°æ§","éš”ç¦»","èˆ†æƒ…","ä¸å½“è¨€è®º","ä¸å®ä¿¡æ¯",
]
ACT_STRONG = [
    "é•‡å‹","æŠ“æ•","æ‹˜ç•™","é€®æ•","åˆ¤å†³","æªå‡»","å¼€æª","å°ç¦","ä¸‹æ¶","åˆ å¸–","å°å·","çº¦è°ˆ","é©±æ•£",
    "æˆ’ä¸¥","å°é”","åˆ‡æ–­","å›´å µ","é©±é€","å¼€é™¤","å…èŒ","æŸ¥å°","åœèŒ","å®¡æŸ¥","å°å­˜","ç¦è¨€","é™æµ",
]

PROMPT = """
ä½ æ˜¯ä¿¡æ¯æŠ½å–åŠ©æ‰‹ï¼Œé¢å‘æ”¿æ²»/å†å²æ•æ„Ÿæ–‡æœ¬ï¼Œæå– SVO æœ‰å‘ä¸‰å…ƒç»„ã€‚
å­—æ®µ: head(ä¸»ä½“/å‘èµ·è€…), relation(ç²¾ç¡®è°“è¯­), tail(å®¢ä½“/æ‰¿å—è€…), direction(active|passive),
type_head/type_tail âˆˆ [Person, Org, Event, Location, Outcome, Unknown]ã€‚
ä»…æŠ½å–ä¸æ•æ„Ÿäº‹ä»¶/é«˜å±‚ä¸»ä½“ç›¸å…³çš„å…³ç³»ï¼šç¾¤ä½“äº‹ä»¶ã€åå…š/é¢ è¦†ã€åˆ†è£‚/ç‹¬ç«‹ã€é‡å¤§ç»´ç¨³/å°ç¦/åˆ é™¤/ä¸‹æ¶/çº¦è°ˆ/æŠ“æ•ã€
å†›æ”¿æœºå¯†/è°ƒåŠ¨ã€æ¶‰å¤–æ‘©æ“¦ã€é«˜å±‚æ–—äº‰ã€åè…å¤§æ¡ˆã€é‡å¤§ç›‘ç®¡/è¡Œä¸šæ•´é¡¿ã€‚
ç¬¬ä¸€äººç§°å™è¿°è‹¥æ¶‰åŠä¸Šè¿°æ•æ„Ÿäº‹ä»¶æˆ–é«˜å±‚ä¸»ä½“ï¼Œä¹Ÿåº”ä¿ç•™ï¼›æ—¥å¸¸ç¤¼èŠ‚æˆ–çäº‹å¯å¿½ç•¥ã€‚
è‹¥æ–‡æœ¬æ— æ•æ„Ÿäº‹ä»¶æˆ–é‡è¦ä¸»ä½“/åŠ¨ä½œï¼Œè¿”å› []ã€‚
æ–¹å‘ï¼šå‡ºç°â€œè¢«/é­/é€®æ•/æ‹˜ç•™/é•‡å‹/å°ç¦/åˆ é™¤â€ç­‰åˆ¤å®š passiveï¼Œå…¶ä½™ activeã€‚
è°“è¯­ä¿ç•™åŸæ–‡åŠ¨è¯ï¼Œä¸ç”¨â€œæ˜¯/æœ‰/è¿›è¡Œ/å¼€å±•â€ç­‰æ³›åŒ–è¯ã€‚
æŒ‰é£é™©å’Œä¸»ä½“çº§åˆ«æ’åºè¾“å‡ºï¼šä¸­å¤®/å†›å§”/å›½å®¶é¢†å¯¼äºº > éƒ¨å§”/çœçº§ > åœ°æ–¹/ä¸ªäººï¼›é«˜æ•æ„Ÿäº‹ä»¶ > ä¸­ > ä½ã€‚

ä»…ä¾æ®ä¸‹åˆ—æ–‡æœ¬ï¼Œä¸è¦ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†ï¼ˆå¯èƒ½è¢«æˆªæ–­ï¼‰ï¼š
{text}
"""

# --- è¾…åŠ©å‡½æ•° ---
def split_text(txt, size=CHUNK_LEN, overlap=OVERLAP):
    out = []
    n = len(txt)
    i = 0
    while i < n:
        out.append(txt[i : i + size])
        i += size - overlap
    return out

def extract_text(file_obj):
    file_name = getattr(file_obj, "name", "") or (file_obj if isinstance(file_obj, str) else "")
    ext = file_name.lower().rsplit(".", 1)[-1] if "." in file_name else ""
    if not ext:
        raise ValueError("ç¼ºå°‘æˆ–ä¸æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å")

    if hasattr(file_obj, "read"):
        data = file_obj.read()
        if hasattr(file_obj, "seek"):
            file_obj.seek(0)
    else:
        with open(file_obj, "rb") as f:
            data = f.read()

    text = ""
    try:
        if ext == "pdf":
            reader = pypdf.PdfReader(io.BytesIO(data))
            for page in reader.pages:
                text += (page.extract_text() or "") + "\n"
        elif ext == "epub":
            with tempfile.NamedTemporaryFile(delete=False, suffix=".epub") as tmp:
                tmp.write(data)
                tmp_path = tmp.name
            try:
                book = epub.read_epub(tmp_path)
                for item in list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT)):
                    soup = BeautifulSoup(item.get_content(), "html.parser")
                    text += soup.get_text() + "\n"
            finally:
                os.remove(tmp_path)
        elif ext in ["docx", "doc"]:
            doc = Document(io.BytesIO(data))
            text = "\n".join([p.text for p in doc.paragraphs])
        else:
            text = data.decode("utf-8", errors="ignore")
    except Exception as e:
        print(f"[extract] {file_name} error: {e}")
    return text

def canonicalize(name: str) -> str:
    if not name:
        return name
    name = name.strip()
    for canon, alias_list in ALIASES.items():
        if name == canon or name in alias_list:
            return canon
    n = "".join(ch for ch in name if ch.isalnum())
    for canon, alias_list in ALIASES.items():
        for a in [canon, *alias_list]:
            if n == "".join(ch for ch in a if ch.isalnum()):
                return canon
    return name

def infer_direction(relation: str, default="active"):
    if not relation:
        return default
    if re.search(r"(è¢«|é­|å—|é€®æ•|æ‹˜ç•™|é•‡å‹|å°ç¦|åˆ é™¤|ä¸‹æ¶|é©±æ•£|å¼€é™¤|å…èŒ|åˆ¶è£)", relation):
        return "passive"
    return default

def score_event(text_chunk: str, relation: str) -> int:
    score = 0
    def has_any(words):
        return any(w in text_chunk or (relation and w in relation) for w in words)
    if has_any(RISK_HIGH):
        score += 3
    elif has_any(RISK_MED):
        score += 2
    if relation and any(w in relation for w in ACT_STRONG):
        score += 1
    return score

def score_actor(name: str) -> int:
    if not name:
        return 0
    central_kw = ["ä¸­å¤®","å›½åŠ¡é™¢","å†›å§”","å…¨å›½äººå¤§","å…¨å›½æ”¿å","ä¸­å®£éƒ¨","ä¸­ç»„éƒ¨","ä¸­çºªå§”","æ”¿æ³•å§”","ç½‘ä¿¡åŠ","å›½å®‰å§”",
                  "æˆ˜åŒº","å†›åŒº","å¸ä»¤éƒ¨","æ€»éƒ¨","éƒ¨å§”","å¤–äº¤éƒ¨","å›½é˜²éƒ¨","å…¬å®‰éƒ¨","å›½å®‰éƒ¨","å‘æ”¹å§”","è´¢æ”¿éƒ¨"]
    prov_kw = ["çœå§”","çœæ”¿åºœ","è‡ªæ²»åŒº","ç›´è¾–å¸‚","çœå†›åŒº","æ­¦è­¦æ€»é˜Ÿ","å…å±€","çœçº§"]
    local_kw = ["å¸‚å§”","å¸‚æ”¿åºœ","å·æ”¿åºœ","å¿å§”","å¿æ”¿åºœ","åŒºå§”","é•‡æ”¿åºœ","è¡—é“","ä¹¡é•‡","æ´¾å‡ºæ‰€","åŸºå±‚"]
    if any(k in name for k in central_kw):
        return 3
    if any(k in name for k in prov_kw):
        return 2
    if any(k in name for k in local_kw):
        return 1
    return 0

@st.cache_resource
def get_client(api_key):
    return genai.Client(api_key=api_key)

def analyze_svo(chunk_data, client, model):
    i, text = chunk_data
    prompt = PROMPT.format(text=text)
    try:
        resp = client.models.generate_content(model=model, contents=prompt)
        raw = resp.text.replace("```json", "").replace("```", "").strip()
        s, e = raw.find("["), raw.rfind("]") + 1
        return json.loads(raw[s:e]) if s != -1 else []
    except Exception as e:
        print(f"[chunk {i}] error: {e}")
        return []

def trim_graph(raw, max_nodes=300, min_nodes=50):
    cnt = Counter()
    for it in raw:
        cnt[it["head"]] += 1
        cnt[it["tail"]] += 1
    top_nodes = {n for n, _ in cnt.most_common(max_nodes)}
    trimmed = [it for it in raw if it["head"] in top_nodes and it["tail"] in top_nodes]
    if len(top_nodes) < min_nodes:
        return raw, False
    if len(top_nodes) > max_nodes:
        return trimmed, True
    return trimmed, False

# --- æ ¸å¿ƒæµç¨‹ ---
def main_run(files, api_key, model):
    chunks = []
    for f in files:
        txt = extract_text(f)
        if len(txt) > 100:
            for i, s in enumerate(split_text(txt)):
                cid = f"{getattr(f,'name',str(f))}-{i}"
                chunks.append((cid, s))

    if not chunks:
        return None, "âŒ æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–è¯»å–å¤±è´¥", False

    st.info(f"ğŸš€ äº‘ç«¯å¼•æ“å¯åŠ¨ï¼šåˆ†æ {len(chunks)} ä¸ªç‰‡æ®µï¼ˆå…¨é‡ï¼Œä¸æˆªæ–­ï¼‰...")
    bar = st.progress(0)
    raw = []

    client = get_client(api_key)
    max_workers = min(MAX_WORKERS, len(chunks))
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as exe:
        futures = [exe.submit(analyze_svo, c, client, model) for c in chunks]
        for i, f in enumerate(concurrent.futures.as_completed(futures)):
            if res := f.result():
                raw.extend(res)
            bar.progress((i + 1) / len(chunks))

    if not raw:
        return None, "âŒ æœªæå–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ API Key æˆ–æ¨¡å‹æƒé™", False

    # å½’ä¸€/è¿‡æ»¤/è¯„åˆ†
    scored = []
    for it in raw:
        h, t, r = canonicalize(it.get("head")), canonicalize(it.get("tail")), it.get("relation")
        if not h or not t or not r:
            continue
        if r in STOP_REL:
            continue
        it["head"], it["tail"] = h, t
        it["direction"] = infer_direction(r, default=it.get("direction", "active"))
        ev_score = score_event("", r)  # å¯æŒ‰éœ€åŠ å…¥ chunk_text
        act_score = max(score_actor(h), score_actor(t))
        total = ev_score + act_score
        it["_score"] = total
        scored.append(it)

    MIN_SCORE = 1
    scored = [it for it in scored if it["_score"] >= MIN_SCORE]
    scored.sort(key=lambda x: x.get("_score", 0), reverse=True)

    # èŠ‚ç‚¹è£å‰ªä»…å½±å“å±•ç¤ºï¼Œä¸å½±å“æŠ½å–
    norm, truncated = trim_graph(scored, max_nodes=300, min_nodes=50)

    # æ„å›¾ï¼ˆä»…çœŸå®æŠ½å–è¾¹ï¼‰
    G = nx.DiGraph()
    for item in norm:
        h, t, r = item["head"], item["tail"], item["relation"]
        ht = item.get("type_head", "Person")
        tt = item.get("type_tail", "Person")
        direction = item.get("direction", "active")
        edge_style = STYLE.get(direction, STYLE["active"])
        G.add_node(h, label=h, color=COLORS.get(ht, "#7c9dff"), size=22)
        G.add_node(t, label=t, color=COLORS.get(tt, "#7c9dff"), size=22)
        label = r if len(r) <= 28 else r[:25] + "..."
        G.add_edge(
            h, t,
            label=label,
            color=edge_style["color"],
            smooth=True,
            arrows="to",
            dashes=edge_style["dashes"],
            weight=3.0
        )

    # ç¤¾åŒºç€è‰²ï¼ˆå¯é€‰ï¼‰
    if HAS_LOUVAIN:
        undi = G.to_undirected()
        part = community_louvain.best_partition(undi, weight="weight")
        palette = ["#4ae0c8","#7c9dff","#c084fc","#22c55e","#f59e0b","#ef4444","#8b5cf6","#0ea5e9"]
        for n, comm in part.items():
            G.nodes[n]["color"] = palette[comm % len(palette)]

    # æŠ¥å‘Š
    rpt = "# DeepGraph Report\n\n"
    rpt += f"- èŠ‚ç‚¹æ•°: {len(G.nodes())}\n- è¾¹æ•°: {len(G.edges())}\n"
    if truncated:
        rpt += "- æ³¨æ„ï¼šèŠ‚ç‚¹å·²æˆªæ–­åˆ°å‰ 300 ä¸ªæœ€ç›¸å…³èŠ‚ç‚¹ï¼ˆä»…å½±å“å±•ç¤ºï¼ŒæŠ½å–æœªæˆªæ–­ï¼‰ã€‚\n"
    rpt += "## é«˜åˆ†å…³ç³»ï¼ˆæŒ‰é£é™©/ä¸»ä½“åˆ†æ’åºï¼Œå‰ 200 æ¡ï¼‰\n"
    for it in scored[:200]:
        rpt += f"[{it.get('_score',0)}] {it['head']} --[{it['relation']}]--> {it['tail']} ({it.get('direction','active')})\n"

    return G, rpt, truncated

# --- ç•Œé¢ ---
st.title("DeepGraph Pro Â· Cloud Edition")

with st.sidebar:
    st.header("Settings")
    st.success("âœ… äº‘ç«¯ç¯å¢ƒå·²å°±ç»ª")
    api_key = st.text_input("Google API Key", type="password")
    model_id = st.text_input("Model ID", value="gemini-2.0-flash-exp")
    if st.button("ğŸ” Check Available Models"):
        if not api_key:
            st.error("Please enter API Key first")
        else:
            try:
                client = genai.Client(api_key=api_key)
                models = [m.name for m in client.models.list() if "gemini" in m.name]
                st.write(models)
            except Exception as e:
                st.error(f"Error: {e}")

col1, col2 = st.columns([1, 2.2])

with col1:
    st.markdown('<div class="glass-card">', unsafe_allow_html=True)
    files = st.file_uploader("Upload Files (PDF/DOCX/TXT)", accept_multiple_files=True)
    st.markdown("<br>", unsafe_allow_html=True)
    start = st.button("ğŸš€ Start Analysis")
    st.markdown("</div>", unsafe_allow_html=True)

    if st.session_state.processed:
        st.download_button("Download Graph HTML", st.session_state.graph_html, "graph.html", "text/html")
        st.download_button("Download Report TXT", st.session_state.report_txt, "report.txt", "text/plain")

with col2:
    status = "Ready"
    if start:
        status = "Running"
    if st.session_state.processed:
        status = "Done"
    st.markdown(
        f"""
        <div class='glass-card' style='padding:12px 16px; display:flex; gap:10px; align-items:center;'>
          <span style='padding:6px 12px; border-radius:999px; background:rgba(74,224,200,0.18); color:#4ae0c8; font-weight:800;'>{status}</span>
          <span style='color:#cbd5e1;'>äº‘ç«¯ SVO å›¾è°±åˆ†æï¼ˆæ•æ„Ÿä¼˜å…ˆ Â· é«˜é€Ÿæ¨¡å¼ï¼‰</span>
        </div>
        """,
        unsafe_allow_html=True,
    )

    if start:
        if not api_key or not files:
            st.error("è¯·å¡«å…¥ API Key å¹¶ä¸Šä¼ æ–‡ä»¶")
        else:
            with st.spinner("Analyzing on Cloud..."):
                G, rpt, truncated = main_run(files, api_key, model_id)
                if G:
                    net = Network(
                        height="820px",
                        width="100%",
                        bgcolor="#0c1224",
                        font_color="#e6edf7",
                        directed=True,
                    )
                    net.from_nx(G)
                    net.set_options("""
{
  "physics": {
    "enabled": true,
    "solver": "forceAtlas2Based",
    "forceAtlas2Based": {
      "gravitationalConstant": -160,
      "centralGravity": 0.01,
      "springLength": 110,
      "springConstant": 0.11,
      "damping": 0.9,
      "avoidOverlap": 1.0
    },
    "stabilization": { "enabled": true, "iterations": 1500, "updateInterval": 30 }
  },
  "edges": { "smooth": false },
  "layout": { "improvedLayout": true },
  "interaction": { "dragNodes": true, "hover": true, "navigationButtons": true }
}
                    """)
                    st.session_state.graph_html = net.generate_html()
                    st.session_state.report_txt = rpt
                    st.session_state.processed = True
                    st.session_state.truncated = truncated
                    st.rerun()
                elif rpt:
                    st.error(rpt)

    if st.session_state.processed:
        if st.session_state.truncated:
            st.warning("âš ï¸ èŠ‚ç‚¹å·²æˆªæ–­è‡³å‰ 300 ä¸ªæœ€ç›¸å…³èŠ‚ç‚¹ï¼ˆä»…å½±å“å±•ç¤ºï¼ŒæŠ½å–æœªæˆªæ–­ï¼‰")
        st.components.v1.html(st.session_state.graph_html, height=820)
