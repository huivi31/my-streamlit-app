import streamlit as st
import os, json, concurrent.futures, io, tempfile, re
from collections import Counter
import pypdf
from docx import Document
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from google import genai
from pyvis.network import Network
import networkx as nx

try:
    import community as community_louvain
    HAS_LOUVAIN = True
except:
    HAS_LOUVAIN = False

st.set_page_config(page_title="DeepGraph Pro v3", layout="wide", page_icon="ğŸ•¸ï¸")

st.markdown("""
<style>
.stApp { background: linear-gradient(145deg, #0c1224, #0f1b2f); color: #e6edf7; }
.stButton > button { background: linear-gradient(120deg, #4ae0c8, #7c6bff); color: #fff; border: none; border-radius: 12px; }
.entity-tag { display: inline-block; padding: 2px 8px; border-radius: 4px; margin: 2px; font-size: 0.85em; }
.entity-person { background: #ff6b6b33; color: #ff6b6b; }
.entity-event { background: #f59e0b33; color: #f59e0b; }
.entity-org { background: #22c55e33; color: #22c55e; }
</style>
""", unsafe_allow_html=True)

# ============================================
# Session State
# ============================================
if "processed" not in st.session_state:
    st.session_state.processed = False
if "graph_html" not in st.session_state:
    st.session_state.graph_html = ""
if "report" not in st.session_state:
    st.session_state.report = ""
if "entities" not in st.session_state:
    st.session_state.entities = []
if "relations" not in st.session_state:
    st.session_state.relations = []

MAX_WORKERS = 6
CHUNK_SIZE = 3500

# ============================================
# Schemaå®šä¹‰ - æ—¶æ”¿å†å²ä¸“ç”¨
# ============================================

ENTITY_TYPES = {
    "Person": "æ”¿æ²»äººç‰©ã€å†å²äººç‰©ã€é¢†å¯¼äººã€é©å‘½è€…ã€å­¦è€…",
    "Event": "æ”¿æ²»äº‹ä»¶ã€å†å²äº‹ä»¶ã€è¿åŠ¨ã€æˆ˜äº‰ã€ä¼šè®®",
    "Organization": "æ”¿å…šã€æ”¿åºœæœºæ„ã€å†›é˜Ÿã€å›½é™…ç»„ç»‡",
    "Policy": "æ”¿ç­–ã€æ³•è§„ã€åˆ¶åº¦ã€æ–¹é’ˆ",
    "Ideology": "æ€æƒ³ã€ç†è®ºã€ä¸»ä¹‰ã€å­¦è¯´",
    "Location": "å›½å®¶ã€åœ°åŒºã€åŸå¸‚ã€é‡è¦åœ°ç‚¹",
    "Time": "å…·ä½“æ—¶é—´ç‚¹æˆ–æ—¶é—´æ®µ",
}

RELATION_TYPES = [
    # äººç‰©å…³ç³»
    "é¢†å¯¼", "ç»§ä»»", "å‰ä»»", "ä¸‹å±", "åŒäº‹", "å¯¹ç«‹",
    # äº‹ä»¶å…³ç³»  
    "å‘èµ·", "å‚ä¸", "ä¸»å¯¼", "åå¯¹", "æ”¯æŒ", "é•‡å‹",
    # å› æœå…³ç³»
    "å¯¼è‡´", "å¼•å‘", "æºäº", "ç»“æŸäº",
    # è¯„ä»·å…³ç³»
    "æ‰¹è¯„", "èµæ‰¬", "å®šæ€§ä¸º", "è¯„ä»·ä¸º",
    # å½’å±å…³ç³»
    "å±äº", "éš¶å±", "åŒ…å«", "ä½äº",
    # æ—¶é—´å…³ç³»
    "å‘ç”Ÿäº", "å¼€å§‹äº", "ç»“æŸäº", "æŒç»­",
]

# ============================================
# é˜¶æ®µä¸€ï¼šå®ä½“æŠ½å– Prompt
# ============================================

ENTITY_PROMPT = """ä½ æ˜¯ä¸“ä¸šçš„æ—¶æ”¿å†å²æ–‡æ¡£åˆ†æä¸“å®¶ã€‚ä»æ–‡æœ¬ä¸­è¯†åˆ«é‡è¦å®ä½“ã€‚

## å®ä½“ç±»å‹å®šä¹‰
- Person: æ”¿æ²»äººç‰©ã€å†å²äººç‰©ã€é¢†å¯¼äºº
- Event: æ”¿æ²»äº‹ä»¶ã€å†å²äº‹ä»¶ã€è¿åŠ¨ã€æˆ˜äº‰ã€ä¼šè®®
- Organization: æ”¿å…šã€æ”¿åºœæœºæ„ã€å†›é˜Ÿã€å›½é™…ç»„ç»‡
- Policy: æ”¿ç­–ã€æ³•è§„ã€åˆ¶åº¦ã€æ–¹é’ˆ
- Ideology: æ€æƒ³ã€ç†è®ºã€ä¸»ä¹‰ã€å­¦è¯´
- Location: é‡è¦åœ°ç‚¹ï¼ˆéæ™®é€šåœ°åï¼‰
- Time: å…³é”®æ—¶é—´ç‚¹æˆ–æ—¶é—´æ®µ

## æŠ½å–è§„åˆ™
1. åªæŠ½å–ä¸æ—¶æ”¿å†å²ç›¸å…³çš„**é‡è¦**å®ä½“
2. è¿‡æ»¤æ—¥å¸¸ç”Ÿæ´»ã€ä¸€èˆ¬æè¿°ä¸­çš„æ™®é€šè¯æ±‡
3. äººåéœ€å®Œæ•´ï¼ˆå¦‚"æ¯›æ³½ä¸œ"è€Œé"æ¯›"ï¼‰
4. äº‹ä»¶åç§°éœ€è§„èŒƒï¼ˆå¦‚"å…­å››äº‹ä»¶"è€Œé"é‚£ä»¶äº‹"ï¼‰

## ç¤ºä¾‹
æ–‡æœ¬ï¼š"1978å¹´12æœˆï¼Œé‚“å°å¹³ä¸»æŒå¬å¼€åä¸€å±Šä¸‰ä¸­å…¨ä¼šï¼Œæ­£å¼ç¡®ç«‹æ”¹é©å¼€æ”¾æ”¿ç­–ã€‚"
è¾“å‡ºï¼š
```json
[
  {"name": "é‚“å°å¹³", "type": "Person"},
  {"name": "åä¸€å±Šä¸‰ä¸­å…¨ä¼š", "type": "Event"},
  {"name": "æ”¹é©å¼€æ”¾", "type": "Policy"},
  {"name": "1978å¹´12æœˆ", "type": "Time"}
]
```

## å¾…åˆ†ææ–‡æœ¬
{text}

## è¾“å‡º
ä»…è¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""

# ============================================
# é˜¶æ®µäºŒï¼šå…³ç³»æŠ½å– Prompt
# ============================================

RELATION_PROMPT = """ä½ æ˜¯ä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚æ ¹æ®æ–‡æœ¬å†…å®¹ï¼Œåœ¨ç»™å®šå®ä½“ä¹‹é—´å»ºç«‹å…³ç³»ã€‚

## å·²è¯†åˆ«çš„å®ä½“
{entities}

## å…³ç³»ç±»å‹å‚è€ƒ
{relation_types}

## æŠ½å–è§„åˆ™
1. åªåœ¨ä¸Šè¿°å®ä½“ä¹‹é—´å»ºç«‹å…³ç³»
2. å…³ç³»å¿…é¡»æœ‰æ˜ç¡®çš„æ–¹å‘ï¼ˆsource â†’ relation â†’ targetï¼‰
3. å…³ç³»æè¿°è¦ç®€æ´ï¼ˆ2-6ä¸ªå­—ï¼‰
4. å¯ä»¥ä½¿ç”¨å‚è€ƒå…³ç³»ç±»å‹ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰æ›´å‡†ç¡®çš„æè¿°
5. æ¯æ¡å…³ç³»å¯é™„å¸¦ç»†èŠ‚è¯´æ˜

## ç¤ºä¾‹
æ–‡æœ¬ï¼š"é‚“å°å¹³ä¸»æŒå¬å¼€åä¸€å±Šä¸‰ä¸­å…¨ä¼šï¼Œç¡®ç«‹äº†æ”¹é©å¼€æ”¾æ”¿ç­–ã€‚"
å®ä½“ï¼šé‚“å°å¹³(Person), åä¸€å±Šä¸‰ä¸­å…¨ä¼š(Event), æ”¹é©å¼€æ”¾(Policy)
è¾“å‡ºï¼š
```json
[
  {{"source": "é‚“å°å¹³", "relation": "ä¸»æŒå¬å¼€", "target": "åä¸€å±Šä¸‰ä¸­å…¨ä¼š", "detail": "1978å¹´12æœˆ"}},
  {{"source": "åä¸€å±Šä¸‰ä¸­å…¨ä¼š", "relation": "ç¡®ç«‹", "target": "æ”¹é©å¼€æ”¾", "detail": "ç»æµä½“åˆ¶æ”¹é©"}}
]
```

## å¾…åˆ†ææ–‡æœ¬
{text}

## è¾“å‡º
ä»…è¿”å›JSONæ•°ç»„ï¼š"""

# ============================================
# å·¥å…·å‡½æ•°
# ============================================

def split_text(text, size=CHUNK_SIZE):
    """åˆ†å—ï¼Œä¿æŒæ®µè½å®Œæ•´æ€§"""
    paragraphs = re.split(r'\n\s*\n', text)
    chunks, current = [], ""
    for p in paragraphs:
        p = p.strip()
        if not p:
            continue
        if len(current) + len(p) < size:
            current += "\n\n" + p if current else p
        else:
            if current:
                chunks.append(current)
            current = p
    if current:
        chunks.append(current)
    return chunks or [text[:size]]

def read_file(f):
    """è¯»å–å„ç§æ ¼å¼çš„æ–‡ä»¶"""
    name = getattr(f, "name", "")
    ext = name.lower().rsplit(".", 1)[-1] if "." in name else ""
    data = f.read()
    if hasattr(f, "seek"):
        f.seek(0)
    
    text = ""
    try:
        if ext == "pdf":
            for page in pypdf.PdfReader(io.BytesIO(data)).pages:
                text += (page.extract_text() or "") + "\n"
        elif ext == "epub":
            with tempfile.NamedTemporaryFile(delete=False, suffix=".epub") as tmp:
                tmp.write(data)
                path = tmp.name
            try:
                book = epub.read_epub(path)
                for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):
                    text += BeautifulSoup(item.get_content(), "html.parser").get_text() + "\n"
            finally:
                os.remove(path)
        elif ext in ["docx", "doc"]:
            text = "\n".join(p.text for p in Document(io.BytesIO(data)).paragraphs)
        else:
            text = data.decode("utf-8", errors="ignore")
    except Exception as e:
        st.error(f"è¯»å–å¤±è´¥: {e}")
    return text

@st.cache_resource
def get_client(key):
    return genai.Client(api_key=key)

def call_llm(client, model, prompt):
    """è°ƒç”¨LLMå¹¶è§£æJSON"""
    try:
        resp = client.models.generate_content(model=model, contents=prompt)
        raw = resp.text.replace("```json", "").replace("```", "").strip()
        start, end = raw.find("["), raw.rfind("]") + 1
        if start >= 0 and end > start:
            return json.loads(raw[start:end])
    except Exception as e:
        print(f"LLM Error: {e}")
    return []

# ============================================
# ä¸¤é˜¶æ®µæŠ½å–
# ============================================

def extract_entities(chunk, client, model):
    """é˜¶æ®µä¸€ï¼šæŠ½å–å®ä½“"""
    prompt = ENTITY_PROMPT.format(text=chunk)
    entities = call_llm(client, model, prompt)
    # è¿‡æ»¤æ— æ•ˆå®ä½“
    valid = []
    for e in entities:
        name = e.get("name", "").strip()
        etype = e.get("type", "")
        if len(name) >= 2 and etype in ENTITY_TYPES:
            valid.append({"name": name, "type": etype})
    return valid

def extract_relations(chunk, entities, client, model):
    """é˜¶æ®µäºŒï¼šæŠ½å–å…³ç³»"""
    if not entities:
        return []
    
    # æ„å»ºå®ä½“æè¿°
    entity_desc = ", ".join([f"{e['name']}({e['type']})" for e in entities])
    relation_desc = ", ".join(RELATION_TYPES)
    
    prompt = RELATION_PROMPT.format(
        text=chunk,
        entities=entity_desc,
        relation_types=relation_desc
    )
    
    relations = call_llm(client, model, prompt)
    
    # éªŒè¯å…³ç³»çš„sourceå’Œtargetéƒ½åœ¨å®ä½“åˆ—è¡¨ä¸­
    entity_names = {e["name"] for e in entities}
    valid = []
    for r in relations:
        src = r.get("source", "").strip()
        tgt = r.get("target", "").strip()
        rel = r.get("relation", "").strip()
        if src and tgt and rel and src in entity_names and tgt in entity_names:
            valid.append({
                "source": src,
                "relation": rel,
                "target": tgt,
                "detail": r.get("detail", "")
            })
    return valid

def deduplicate_entities(all_entities):
    """å®ä½“å»é‡"""
    seen = {}
    for e in all_entities:
        name = e["name"]
        if name not in seen:
            seen[name] = e
    return list(seen.values())

def deduplicate_relations(all_relations):
    """å…³ç³»å»é‡"""
    seen = set()
    unique = []
    for r in all_relations:
        key = f"{r['source']}|{r['relation']}|{r['target']}"
        if key not in seen:
            seen.add(key)
            unique.append(r)
    return unique

# ============================================
# å›¾è°±æ„å»º
# ============================================

def build_graph(entities, relations):
    """æ„å»ºæœ‰å‘å›¾"""
    G = nx.DiGraph()
    
    # å®ä½“ç±»å‹é¢œè‰²
    type_colors = {
        "Person": "#ff6b6b",
        "Event": "#f59e0b",
        "Organization": "#22c55e",
        "Policy": "#06b6d4",
        "Ideology": "#c084fc",
        "Location": "#7c9dff",
        "Time": "#94a3b8",
    }
    
    # ç»Ÿè®¡å®ä½“å‡ºç°æ¬¡æ•°ï¼ˆç”¨äºèŠ‚ç‚¹å¤§å°ï¼‰
    entity_degree = Counter()
    for r in relations:
        entity_degree[r["source"]] += 1
        entity_degree[r["target"]] += 1
    
    max_degree = max(entity_degree.values()) if entity_degree else 1
    
    # æ·»åŠ èŠ‚ç‚¹
    entity_map = {e["name"]: e for e in entities}
    for name, e in entity_map.items():
        degree = entity_degree.get(name, 0)
        size = 15 + (degree / max_degree) * 40  # 15-55
        color = type_colors.get(e["type"], "#94a3b8")
        
        G.add_node(name, 
                   label=name, 
                   color=color, 
                   size=size,
                   title=f"{name}\nç±»å‹: {e['type']}\nå…³è”æ•°: {degree}")
    
    # æ·»åŠ è¾¹
    for r in relations:
        src, tgt = r["source"], r["target"]
        if not G.has_node(src):
            G.add_node(src, label=src, color="#94a3b8", size=15)
        if not G.has_node(tgt):
            G.add_node(tgt, label=tgt, color="#94a3b8", size=15)
        
        # è¾¹æ ‡ç­¾
        label = r["relation"]
        title = f"{src} â†’ {r['relation']} â†’ {tgt}"
        if r.get("detail"):
            title += f"\n{r['detail']}"
        
        G.add_edge(src, tgt, 
                   label=label,
                   title=title,
                   arrows="to",
                   color="#4ae0c8aa")
    
    return G

# ============================================
# ä¸»æµç¨‹
# ============================================

def run(files, api_key, model, progress_callback=None):
    client = get_client(api_key)
    
    # è¯»å–æ‰€æœ‰æ–‡ä»¶
    all_text = ""
    for f in files:
        all_text += read_file(f) + "\n\n"
    
    if len(all_text.strip()) < 100:
        return None, None, "æ–‡ä»¶å†…å®¹è¿‡å°‘"
    
    # åˆ†å—
    chunks = split_text(all_text)
    total_steps = len(chunks) * 2  # ä¸¤é˜¶æ®µ
    current_step = 0
    
    st.info(f"ğŸ“„ å…± {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œå¼€å§‹ä¸¤é˜¶æ®µæŠ½å–...")
    
    # é˜¶æ®µä¸€ï¼šå®ä½“æŠ½å–
    st.write("**é˜¶æ®µä¸€ï¼šå®ä½“è¯†åˆ«**")
    bar1 = st.progress(0, text="æŠ½å–å®ä½“ä¸­...")
    all_entities = []
    
    for i, chunk in enumerate(chunks):
        entities = extract_entities(chunk, client, model)
        all_entities.extend(entities)
        current_step += 1
        bar1.progress(current_step / total_steps, text=f"å®ä½“æŠ½å– {i+1}/{len(chunks)}")
    
    # å®ä½“å»é‡
    all_entities = deduplicate_entities(all_entities)
    st.success(f"âœ… è¯†åˆ«åˆ° {len(all_entities)} ä¸ªå”¯ä¸€å®ä½“")
    
    # é˜¶æ®µäºŒï¼šå…³ç³»æŠ½å–
    st.write("**é˜¶æ®µäºŒï¼šå…³ç³»æŠ½å–**")
    bar2 = st.progress(0, text="æŠ½å–å…³ç³»ä¸­...")
    all_relations = []
    
    for i, chunk in enumerate(chunks):
        # åªç”¨è¿™ä¸ªchunkç›¸å…³çš„å®ä½“
        relations = extract_relations(chunk, all_entities, client, model)
        all_relations.extend(relations)
        current_step += 1
        bar2.progress(current_step / total_steps, text=f"å…³ç³»æŠ½å– {i+1}/{len(chunks)}")
    
    # å…³ç³»å»é‡
    all_relations = deduplicate_relations(all_relations)
    st.success(f"âœ… æŠ½å–åˆ° {len(all_relations)} æ¡å”¯ä¸€å…³ç³»")
    
    if not all_relations:
        return None, None, "æœªæŠ½å–åˆ°æœ‰æ•ˆå…³ç³»"
    
    # æ„å»ºå›¾è°±
    G = build_graph(all_entities, all_relations)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(all_entities, all_relations)
    
    return G, (all_entities, all_relations), report

def generate_report(entities, relations):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report = "# çŸ¥è¯†å›¾è°±æŠ½å–æŠ¥å‘Š\n\n"
    report += f"## ğŸ“Š ç»Ÿè®¡\n"
    report += f"- å®ä½“æ•°é‡: {len(entities)}\n"
    report += f"- å…³ç³»æ•°é‡: {len(relations)}\n\n"
    
    # æŒ‰ç±»å‹åˆ†ç»„å®ä½“
    report += "## ğŸ·ï¸ å®ä½“åˆ—è¡¨\n\n"
    by_type = {}
    for e in entities:
        t = e["type"]
        if t not in by_type:
            by_type[t] = []
        by_type[t].append(e["name"])
    
    for t, names in by_type.items():
        report += f"### {t} ({len(names)})\n"
        report += ", ".join(names) + "\n\n"
    
    # å…³ç³»åˆ—è¡¨
    report += "## ğŸ”— å…³ç³»åˆ—è¡¨\n\n"
    for r in relations:
        detail = f" *({r['detail']})*" if r.get("detail") else ""
        report += f"- {r['source']} â†’ **{r['relation']}** â†’ {r['target']}{detail}\n"
    
    return report

# ============================================
# ç•Œé¢
# ============================================

st.title("ğŸ•¸ï¸ DeepGraph Pro v3")
st.caption("ä¸¤é˜¶æ®µçŸ¥è¯†å›¾è°±æŠ½å– | æ—¶æ”¿å†å²ä¸“ç”¨")

with st.sidebar:
    st.subheader("âš™ï¸ é…ç½®")
    api_key = st.text_input("API Key", type="password")
    model = st.text_input("Model", value="gemini-2.0-flash-exp")
    
    st.divider()
    st.subheader("ğŸ“‹ Schema")
    with st.expander("å®ä½“ç±»å‹"):
        for t, desc in ENTITY_TYPES.items():
            st.write(f"**{t}**: {desc}")
    with st.expander("å…³ç³»ç±»å‹"):
        st.write(", ".join(RELATION_TYPES))

# ä¸»ç•Œé¢ä½¿ç”¨tabs
tab1, tab2, tab3 = st.tabs(["ğŸ“¤ ä¸Šä¼ ", "ğŸ•¸ï¸ å›¾è°±", "ğŸ“Š æŠ¥å‘Š"])

with tab1:
    files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", accept_multiple_files=True, 
                             type=["pdf", "epub", "docx", "txt"])
    
    if st.button("ğŸš€ å¼€å§‹æŠ½å–", type="primary", use_container_width=True):
        if api_key and files:
            with st.container():
                G, data, report = run(files, api_key, model)
                
                if G and data:
                    entities, relations = data
                    
                    # ç”Ÿæˆå¯è§†åŒ–
                    net = Network(height="700px", width="100%", 
                                  bgcolor="#0c1224", font_color="#e6edf7", 
                                  directed=True)
                    net.from_nx(G)
                    net.set_options('''
{
  "physics": {
    "enabled": true,
    "solver": "forceAtlas2Based",
    "forceAtlas2Based": {
      "gravitationalConstant": -80,
      "centralGravity": 0.015,
      "springLength": 120,
      "springConstant": 0.08,
      "damping": 0.85,
      "avoidOverlap": 0.95
    },
    "stabilization": {"enabled": true, "iterations": 300}
  },
  "edges": {
    "smooth": {"type": "continuous"},
    "font": {"size": 11, "color": "#94a3b8", "strokeWidth": 0}
  },
  "interaction": {"hover": true, "navigationButtons": true, "keyboard": true}
}
                    ''')
                    
                    st.session_state.graph_html = net.generate_html()
                    st.session_state.report = report
                    st.session_state.entities = entities
                    st.session_state.relations = relations
                    st.session_state.processed = True
                    st.rerun()
                else:
                    st.error(report)
        else:
            st.warning("è¯·å¡«å†™API Keyå¹¶ä¸Šä¼ æ–‡ä»¶")

with tab2:
    if st.session_state.processed:
        # ç»Ÿè®¡ä¿¡æ¯
        col1, col2, col3 = st.columns(3)
        col1.metric("å®ä½“", len(st.session_state.entities))
        col2.metric("å…³ç³»", len(st.session_state.relations))
        col3.metric("èŠ‚ç‚¹è¿æ¥", 
                    sum(1 for _ in st.session_state.relations))
        
        # å›¾è°±
        st.components.v1.html(st.session_state.graph_html, height=700)
    else:
        st.info("è¯·å…ˆä¸Šä¼ æ–‡æ¡£å¹¶æŠ½å–")

with tab3:
    if st.session_state.processed:
        # ä¸‹è½½æŒ‰é’®
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                "ğŸ“¥ ä¸‹è½½å®ä½“+å…³ç³» JSON",
                json.dumps({
                    "entities": st.session_state.entities,
                    "relations": st.session_state.relations
                }, ensure_ascii=False, indent=2),
                "knowledge_graph.json",
                use_container_width=True
            )
        with col2:
            st.download_button(
                "ğŸ“¥ ä¸‹è½½æŠ¥å‘Š Markdown",
                st.session_state.report,
                "report.md",
                use_container_width=True
            )
        
        st.divider()
        st.markdown(st.session_state.report)
    else:
        st.info("è¯·å…ˆä¸Šä¼ æ–‡æ¡£å¹¶æŠ½å–")
