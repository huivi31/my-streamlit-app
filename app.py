import streamlit as st
import os, json, io, tempfile, re, math, time
from typing import List, Optional
from enum import Enum
from collections import defaultdict
import pypdf
from docx import Document
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
from google import genai
from google.genai import types
from pydantic import BaseModel, Field
from pyvis.network import Network
import networkx as nx

st.set_page_config(page_title="è§£ä¹¦å®¢", layout="wide", page_icon="ğŸ“–", initial_sidebar_state="collapsed")

# ============================================
# å…šå²æ–‡çŒ®ç ”ç©¶é™¢é£æ ¼ - ä¸­å›½çº¢ + åº„é‡ä¸¥è‚ƒ
# ============================================
st.markdown("""

<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@700;900&display=swap');
    /* ========== Global Theme: CPC Official Document ========== */
    :root {
        --china-red: #B71C1C;
        --china-red-hover: #D32F2F;
        --gold-main: #FFD700;
        --gold-accent: #DAA520;
        --text-main: #2C2C2C;
        --text-sub: #555555;
        --bg-page: #FAFAFA;
        --bg-card: #FFFFFF;
        --border-color: #E0E0E0;
        --font-serif: "STFangsong", "FangSong", "SimSun", serif;
    }

    /* Standard Reset & Typography */
    .stApp {
        background-color: var(--bg-page);
        color: var(--text-main);
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    }
    
    h1, h2, h3 {
        font-family: var(--font-serif) !important;
        color: var(--china-red) !important;
        font-weight: bold !important;
    }

    /* ========== Widgets Styling ========== */
    
    /* Buttons: Primary Red Action */
    .stButton > button {
        background: linear-gradient(135deg, var(--china-red) 0%, #8B0000 100%) !important;
        color: #FFD700 !important; /* Gold text */
        border: none !important;
        border-radius: 4px !important;
        padding: 0.6rem 1.2rem !important;
        font-family: var(--font-serif) !important;
        font-size: 1.1rem !important;
        letter-spacing: 0.1em !important;
        box-shadow: 0 4px 6px rgba(139, 0, 0, 0.2);
        transition: all 0.3s ease;
    }
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(139, 0, 0, 0.3);
    }
    
    /* Tabs: Official Document Style */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background-color: transparent;
        border-bottom: 2px solid var(--border-color);
        padding-bottom: 0px;
    }
    .stTabs [data-baseweb="tab"] {
        background-color: transparent !important;
        border: none !important;
        color: var(--text-sub) !important;
        font-family: var(--font-serif) !important;
        font-size: 1.1rem !important;
    }
    .stTabs [aria-selected="true"] {
        color: var(--china-red) !important;
        border-bottom: 3px solid var(--china-red) !important;
        font-weight: bold !important;
        background-color: rgba(183, 28, 28, 0.05) !important;
    }

    /* Text Inputs */
    .stTextInput > div > div > input {
        border: 1px solid var(--border-color);
        border-radius: 4px;
        padding: 0.5rem;
    }
    .stTextInput > div > div > input:focus {
        border-color: var(--china-red) !important;
        box-shadow: 0 0 0 2px rgba(183, 28, 28, 0.2) !important;
    }

    /* Cards / Containers */
    [data-testid="stVerticalBlock"] {
        gap: 1rem;
    }

    /* Sidebar */
    [data-testid="stSidebar"] {
        background-color: #F8F9FA;
        border-right: 1px solid #E0E0E0;
    }
    
    /* Top Header Bar Adjustments */
    header[data-testid="stHeader"] {
        background-color: transparent !important;
    }

    /* ========== CPC Header Banner (Preserved & Optimized) ========== */
    .cpc-header {
        position: relative;
        background: linear-gradient(90deg, #D32F2F 0%, #C62828 50%, #B71C1C 100%);
        padding: 20px 0;
        margin: -6rem -4rem 30px -4rem;
        box-shadow: 0 8px 24px rgba(0,0,0,0.5);
        width: calc(100% + 8rem);
        display: flex;
        align-items: center;
        justify-content: center;
        text-align: center;
        white-space: nowrap;
        overflow: hidden; /* Prevent scrollbar if font is too big */
    }
    
    .cpc-header::before {
        content: '';
        position: absolute;
        top: 0; left: 0; right: 0; bottom: 0;
        background: radial-gradient(circle at 50% 50%, rgba(255, 215, 0, 0.1) 0%, transparent 60%);
        pointer-events: none;
    }
    
    .cpc-header-title {
        font-family: "Noto Serif SC", "Source Han Serif SC", "SimSun", serif;
        font-size: 13vw;
        font-weight: 900;
        margin: 0;
        padding: 0 20px;
        letter-spacing: 0.02em;
        line-height: 1.1;
        
        /* æ¢å¤äº®é‡‘æ¸å˜ */
        background: linear-gradient(180deg, #FFFFE0 10%, #FFD700 40%, #DAA520 70%, #FFD700 90%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        
        /* æŸ”å’Œçš„æ·±çº¢æŠ•å½± + é»‘è‰²è¿œæ™¯è™šåŒ– = æ‚¬æµ®æ„Ÿ */
        filter: drop-shadow(0 2px 0 #800000) drop-shadow(0 5px 10px rgba(0,0,0,0.3));
        
        /* æç»†çš„æ·±è‰²æè¾¹å¢åŠ æ¸…æ™°åº¦ï¼Œä½†ä¸è¿‡åˆ† */
        -webkit-text-stroke: 1px rgba(139, 0, 0, 0.2);
    }
    
    .decorative-stars {
        position: absolute;
        width: 100%; height: 100%;
        top: 0; left: 0; pointer-events: none; z-index: 0;
    }
    
    .star {
        position: absolute;
        color: #FFD700;
        font-size: 24px;
        opacity: 0.5;
        animation: star-pulse 4s ease-in-out infinite;
        filter: drop-shadow(0 0 5px rgba(255, 215, 0, 0.8));
    }
    
    @keyframes star-pulse {
        0%, 100% { opacity: 0.3; transform: scale(1); }
        50% { opacity: 0.7; transform: scale(1.3); }
    }
    
    .star:nth-child(1) { top: 20%; left: 8%; font-size: 28px; animation-delay: 0s; }
    .star:nth-child(2) { top: 70%; left: 5%; font-size: 20px; animation-delay: 1s; }
    .star:nth-child(3) { top: 30%; right: 8%; font-size: 26px; animation-delay: 2s; }
    .star:nth-child(4) { top: 80%; right: 12%; font-size: 18px; animation-delay: 3s; }
</style>

<!-- CPC Header Banner -->
<div class="cpc-header">
    <div class="decorative-stars">
        <div class="star">â˜…</div>
        <div class="star">â˜…</div>
        <div class="star">â˜…</div>
        <div class="star">â˜…</div>
    </div>
    <h1 class="cpc-header-title">å…šæ”¿å†å²æ–‡çŒ®æ™ºèƒ½åŒ–å¤„ç†</h1>
</div>

""", unsafe_allow_html=True)

# ============================================
# Pydantic Schema - Event-Centric Knowledge Graph
# ============================================

class RiskLevel(str, Enum):
    SAFE = "SAFE"               # ç¬¦åˆå®˜æ–¹å™äº‹
    CONTROVERSIAL = "CONTROVERSIAL"  # æœ‰äº‰è®®/æœªå®šè®º
    HIGH_RISK = "HIGH_RISK"     # æ˜æ˜¾è¿è§„/å†å²è™šæ— ä¸»ä¹‰

class EntityType(str, Enum):
    PERSON = "PERSON"           # æ”¿æ²»äººç‰©
    LOCATION = "LOCATION"       # åœ°ç‚¹
    ORG = "ORG"                  # ç»„ç»‡/å…šæ´¾
    DOCUMENT = "DOCUMENT"       # æ–‡ä»¶/è‘—ä½œ/å†³è®®
    CONCEPT = "CONCEPT"         # ææ³•/å£å·/ä¸»ä¹‰

class EventType(str, Enum):
    MEETING = "MEETING"         # ä¼šè®®
    CONFLICT = "CONFLICT"       # æˆ˜äº‰/å†²çª
    SPEECH = "SPEECH"           # è®²è¯/å‘è¡¨
    POLICY = "POLICY"           # æ”¿ç­–å‡ºå°
    MOVEMENT = "MOVEMENT"       # æ”¿æ²»è¿åŠ¨

# ç§»é™¤ RelationType æšä¸¾ï¼Œæ”¹ä¸ºè‡ªç”±æ–‡æœ¬å…³ç³»

class EntityNode(BaseModel):
    """å®ä½“èŠ‚ç‚¹"""
    id: str = Field(..., description="å½’ä¸€åŒ–IDï¼Œå¦‚ 'PER_Mao_Zedong'")
    name: str = Field(..., description="å®ä½“æ ‡å‡†ä¸­æ–‡å")
    type: EntityType
    alias: List[str] = Field(default=[], description="æ–‡ä¸­å‡ºç°çš„åˆ«å/é»‘è¯")

class EventNode(BaseModel):
    """äº‹ä»¶èŠ‚ç‚¹"""
    id: str = Field(..., description="äº‹ä»¶IDï¼Œæ ¼å¼ï¼šEVT_åŠ¨è¯_ä¸»ä½“_æ—¶é—´")
    name: str = Field(..., description="äº‹ä»¶ç®€è¿°ï¼Œå¦‚'éµä¹‰ä¼šè®®å¬å¼€'")
    type: EventType
    time_str: str = Field(..., description="æ ‡å‡†åŒ–æ—¶é—´å­—ç¬¦ä¸² YYYY-MM-DD")
    description: str = Field(..., description="äº‹ä»¶çš„è¯¦ç»†ç»è¿‡æè¿°")
    political_significance: str = Field(..., description="è¯¥äº‹ä»¶çš„æ”¿æ²»å®šæ€§/å†å²æ„ä¹‰")
    risk_level: RiskLevel = Field(..., description="æ ¹æ®è¾“å…¥æºåˆ¤æ–­è¯¥æè¿°çš„é£é™©ç­‰çº§")

class RelationEdge(BaseModel):
    """å…³ç³»è¾¹"""
    source_id: str = Field(..., description="æºèŠ‚ç‚¹ID (Entity æˆ– Event)")
    target_id: str = Field(..., description="ç›®æ ‡èŠ‚ç‚¹ID")
    relation: str = Field(..., description="å…³ç³»åŠ¨è¯/åŠ¨ä½œï¼Œå¦‚ï¼šå‚ä¸ã€ç»„ç»‡ã€å‘èµ·ã€æ‰¹è¯„ã€æ”¯æŒã€åå¯¹ã€ä»»å‘½ã€å‡ºå¸­ã€é¢†å¯¼ã€æå‡ºã€æ‰¹å‡†ã€ç­¾ç½²ã€è°ƒä»»ã€é€®æ•ã€å¤„å†³ã€å¹³åç­‰")
    details: str = Field(..., description="å…³ç³»çš„å…·ä½“ç»†èŠ‚ï¼Œå¦‚'æ‹…ä»»ç»„é•¿'ã€'é€ æˆ300äººä¼¤äº¡'")
    evidence: str = Field(..., description="åŸæ–‡è¯æ®ç‰‡æ®µ")

class HistoricalGraphBatch(BaseModel):
    """å•æ¬¡å¤„ç†è¿”å›çš„å›¾è°±åˆ‡ç‰‡"""
    entities: List[EntityNode]
    events: List[EventNode]
    relations: List[RelationEdge]

ENTITY_TYPE_CN = {
    "PERSON": "äººç‰©", "LOCATION": "åœ°ç‚¹", "ORG": "ç»„ç»‡",
    "DOCUMENT": "æ–‡ä»¶", "CONCEPT": "æ¦‚å¿µ"
}

EVENT_TYPE_CN = {
    "MEETING": "ä¼šè®®", "CONFLICT": "å†²çª", "SPEECH": "è®²è¯",
    "POLICY": "æ”¿ç­–", "MOVEMENT": "è¿åŠ¨"
}

RISK_LEVEL_CN = {
    "SAFE": "ç¬¦åˆå®˜æ–¹å™äº‹", "CONTROVERSIAL": "æœ‰äº‰è®®/æœªå®šè®º", "HIGH_RISK": "æ˜æ˜¾è¿è§„"
}

RELATION_TYPE_CN = {
    "PARTICIPATED_IN": "å‚ä¸", "ORGANIZED": "ç»„ç»‡", "OCCURRED_AT": "å‘ç”Ÿäº",
    "CAUSED": "å¯¼è‡´", "CONTRADICTS": "é©³æ–¥", "DEFINED_AS": "å®šæ€§ä¸º"
}

# ============================================
# Session State
# ============================================
if "step" not in st.session_state:
    st.session_state.step = 1
if "text_content" not in st.session_state:
    st.session_state.text_content = ""
if "chunks" not in st.session_state:
    st.session_state.chunks = []
if "entities" not in st.session_state:
    st.session_state.entities = []
if "events" not in st.session_state:
    st.session_state.events = []
if "relations" not in st.session_state:
    st.session_state.relations = []
if "focus_stats" not in st.session_state:
    st.session_state.focus_stats = {"nodes": 0, "relations": 0}

CHUNK_SIZE = 4000

# ============================================
# File Reading
# ============================================

def ocr_pdf_with_gemini(file_bytes, api_key):
    """Fallback: Batch OCR using Gemini 3 Flash Preview (10 pages per request)"""
    try:
        if not api_key:
            return ""
        
        # Init client
        client = genai.Client(api_key=api_key)
        
        # Load PDF Structure
        try:
            pdf_reader = pypdf.PdfReader(io.BytesIO(file_bytes))
            total_pages = len(pdf_reader.pages)
        except Exception as e:
            st.error(f"PDFè§£æå¤±è´¥ï¼Œæ— æ³•åˆ†å·: {e}")
            return ""

        full_text = ""
        batch_size = 10 # 10 pages fits well within 8k output token limit
        
        # UI Elements
        st.toast(f"å¼€å§‹å…¨é‡è¯†åˆ«ï¼Œå…± {total_pages} é¡µï¼Œå°†åˆ† {math.ceil(total_pages/batch_size)} æ‰¹å¤„ç†...", icon="ğŸ“š")
        status_text = st.empty()
        progress_bar = st.progress(0)
        
        preview_expander = st.expander("å®æ—¶è¯†åˆ«é¢„è§ˆ (åˆ†å·å¤„ç†ä¸­)", expanded=True)
        preview_area = preview_expander.empty()
        
        # Strict Model
        model_name = "gemini-3-flash-preview"

        for start_idx in range(0, total_pages, batch_size):
            end_idx = min(start_idx + batch_size, total_pages)
            status_text.markdown(f"ğŸš€ æ­£åœ¨å¤„ç†ç¬¬ **{start_idx+1} - {end_idx}** é¡µ (å…± {total_pages} é¡µ)...")
            
            # Create Batch PDF
            writer = pypdf.PdfWriter()
            # If start_idx not 0, careful with resource referencing if strict, but pypdf usually handles well
            for i in range(start_idx, end_idx):
                writer.add_page(pdf_reader.pages[i])
            
            with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_batch:
                writer.write(tmp_batch)
                batch_path = tmp_batch.name
                
            try:
                # Upload
                uploaded_file = client.files.upload(file=batch_path)
                
                # Processing Wait
                while uploaded_file.state.name == "PROCESSING":
                    time.sleep(1)
                    uploaded_file = client.files.get(name=uploaded_file.name)
                
                if uploaded_file.state.name == "FAILED":
                    st.error(f"åˆ†å· {start_idx}-{end_idx} ä¸Šä¼ å¤„ç†å¤±è´¥ï¼Œè·³è¿‡ã€‚")
                    continue

                # Generate Stream
                response_stream = client.models.generate_content_stream(
                    model=model_name,
                    contents=[
                        uploaded_file,
                        "Please transcribe the full text content of this document verbatim. Do not summarize. Just output the text content found in the document."
                    ]
                )
                
                batch_text = ""
                for chunk in response_stream:
                    if chunk.text:
                        batch_text += chunk.text
                        # Preview: Show tail of full text + current batch growing
                        display_text = (full_text[-200:] if full_text else "") + "\n[...æ¥ä¸Šæ–‡...]\n" + batch_text
                        preview_area.markdown(display_text[-1000:] + "...")
                
                full_text += batch_text + "\n"
                
            except Exception as e:
                st.warning(f"âš ï¸ åˆ†å· {start_idx}-{end_idx} è¯†åˆ«å‡ºç°é—®é¢˜: {e}")
                # Don't break, try next batch
            finally:
                if os.path.exists(batch_path):
                    os.remove(batch_path)
            
            # Update Progress
            progress_bar.progress(end_idx / total_pages)

        status_text.success(f"âœ… å…¨æ–‡æ¡£å¤„ç†å®Œæˆï¼å…± {len(full_text)} å­—")
        time.sleep(1)
        status_text.empty()
        progress_bar.empty()
        return full_text

    except Exception as e:
        st.error(f"OCR æµç¨‹è‡´å‘½é”™è¯¯: {e}")
        return ""


def read_file(f, api_key=None):
    name = getattr(f, "name", "")
    ext = name.lower().rsplit(".", 1)[-1] if "." in name else ""
    data = f.read()
    if hasattr(f, "seek"):
        f.seek(0)
    
    text = ""
    try:
        if ext == "pdf":
            # 1. Try PyPDF first (Fast & Cheap)
            try:
                for page in pypdf.PdfReader(io.BytesIO(data)).pages:
                    text += (page.extract_text() or "") + "\n"
            except:
                pass # Fail silently to trigger fallback
            
            # 2. Check sufficiency & Trigger OCR
            # Enhanced logic: Raise threshold to 500 to catch watermarked scanned PDFs
            clean_text = text.strip()
            if len(clean_text) < 500:
                if api_key:
                    # Provide feedback to user
                    st.toast(f"æ­£åœ¨ä½¿ç”¨ Gemini Vision è¯†åˆ«æ‰«ææ–‡æ¡£: {name}...", icon="ğŸ‘ï¸")
                    ocr_text = ocr_pdf_with_gemini(data, api_key)
                    
                    # Accept OCR result if it's substantial
                    if ocr_text and len(ocr_text) > 100:
                        text = ocr_text
                else:
                    if len(clean_text) < 100:
                        st.warning(f"æ–‡ä»¶ {name} ä¼¼ä¹æ˜¯æ‰«æç‰ˆPDFï¼Œéœ€è¦ API Key æ‰èƒ½è¿›è¡Œ OCR è¯†åˆ«ã€‚")

        elif ext == "epub":
            with tempfile.NamedTemporaryFile(delete=False, suffix=".epub") as tmp:
                tmp.write(data)
                path = tmp.name
            try:
                book = epub.read_epub(path, options={'ignore_ncx': True})
                for item in book.get_items():
                    if item.get_type() == ebooklib.ITEM_DOCUMENT:
                        soup = BeautifulSoup(item.get_content(), "html.parser")
                        for tag in soup(['script', 'style']):
                            tag.decompose()
                        text += soup.get_text(separator='\n', strip=True) + "\n"
            finally:
                if os.path.exists(path):
                    os.remove(path)
        elif ext in ["docx", "doc"]:
            text = "\n".join(p.text for p in Document(io.BytesIO(data)).paragraphs)
        else:
            text = data.decode("utf-8", errors="ignore")
    except Exception as e:
        st.error(f"è¯»å–å¤±è´¥: {e}")
    return re.sub(r'\n{3,}', '\n\n', text).strip()

def split_text_simple(text, size=CHUNK_SIZE):
    """ç®€å•åˆ‡åˆ†ï¼ˆå¤‡ç”¨ï¼‰"""
    paragraphs = re.split(r'\n\s*\n', text)
    chunks, current = [], ""
    for p in paragraphs:
        p = p.strip()
        if not p:
            continue
        if len(current) + len(p) < size:
            current += "\n\n" + p if current else p
        else:
            if current:
                chunks.append(current)
            current = p
    if current:
        chunks.append(current)
    return chunks or [text[:size]]

# ============================================
# LLM Client
# ============================================
@st.cache_resource
def get_client(key):
    return genai.Client(api_key=key)

# ============================================
# äº‹ä»¶åˆ‡åˆ†å…³é”®è¯ï¼ˆè§„åˆ™ä¼˜å…ˆ + å°‘é‡ LLM æ ¡éªŒï¼‰
# ============================================
EVENT_BREAK_KEYWORDS = [
    # æ—¶é—´è·³è·ƒ
    "ç¬¬äºŒå¤©", "æ¬¡æ—¥", "ç¿Œæ—¥", "å‡ å¤©å", "æ•°æ—¥å", "ä¸€å‘¨å", "å‡ å‘¨å",
    "ä¸€ä¸ªæœˆå", "æ•°æœˆå", "åŠå¹´å", "ä¸€å¹´å", "å¤šå¹´å", "è‹¥å¹²å¹´å",
    "è½¬çœ¼", "ä¸ä¹…", "éšå", "æ­¤å", "åæ¥", "æœ€ç»ˆ", "ç»ˆäº",
    # ç©ºé—´è·³è·ƒ
    "ä¸æ­¤åŒæ—¶", "å¦ä¸€è¾¹", "åœ¨å¦ä¸€å¤„", "åœ¨åŒ—äº¬", "åœ¨ä¸Šæµ·", "åœ¨å»¶å®‰",
    "å›åˆ°", "æ¥åˆ°", "æŠµè¾¾", "å‰å¾€", "ç¦»å¼€",
    # æ–°äº‹ä»¶æ ‡å¿—
    "ä¼šè®®å¼€å§‹", "ä¼šè®®å¬å¼€", "å¤§ä¼šå¼€å¹•", "ä¼šä¸Š", "ä¼šå",
    "æˆ˜æ–—æ‰“å“", "æˆ˜å½¹å¼€å§‹", "å†²çªçˆ†å‘",
    "å‘è¡¨è®²è¯", "ä½œæŠ¥å‘Š", "å‘è¨€æŒ‡å‡º", "å®£å¸ƒ",
    "é¢å¸ƒ", "å‡ºå°", "é€šè¿‡å†³è®®", "ç­¾ç½²",
    # ç« èŠ‚æ ‡è®°
    "ç¬¬ä¸€ç« ", "ç¬¬äºŒç« ", "ç¬¬ä¸‰ç« ", "ç¬¬å››ç« ", "ç¬¬äº”ç« ",
    "ä¸€ã€", "äºŒã€", "ä¸‰ã€", "å››ã€", "äº”ã€",
    "ï¼ˆä¸€ï¼‰", "ï¼ˆäºŒï¼‰", "ï¼ˆä¸‰ï¼‰", "ï¼ˆå››ï¼‰", "ï¼ˆäº”ï¼‰",
]

BREAKPOINT_PROMPT = """åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªæ®µè½ä¹‹é—´æ˜¯å¦å‘ç”Ÿäº†ã€æ˜æ˜¾çš„äº‹ä»¶è½¬ç§»ã€‘æˆ–ã€æ—¶é—´/åœ°ç‚¹çš„å¤§å¹…è·³è·ƒã€‘ã€‚

ä¸Šä¸€æ®µçš„ç»“å°¾: "...{prev_end}"
ä¸‹ä¸€æ®µçš„å¼€å¤´: "{next_start}..."

å¦‚æœæ˜¯åŒä¸€ä¸ªäº‹ä»¶çš„å»¶ç»­ï¼Œè¾“å‡º NOã€‚
å¦‚æœæ˜¯æ–°çš„äº‹ä»¶å¼€å§‹ï¼Œè¾“å‡º YESã€‚
åªè¾“å‡º YES æˆ– NOã€‚"""

def is_obvious_break(para_start: str) -> bool:
    """è§„åˆ™åˆ¤æ–­ï¼šæ˜¯å¦ä¸ºæ˜æ˜¾çš„äº‹ä»¶æ–­ç‚¹"""
    for kw in EVENT_BREAK_KEYWORDS:
        if kw in para_start[:50]:
            return True
    return False

def fast_event_chunker(
    book_content: str, 
    min_chunk_size: int = 800,
    max_chunk_size: int = 3000
) -> List[str]:
    """
    å¿«é€Ÿåˆ‡åˆ†ï¼šçº¯è§„åˆ™ï¼Œæ—  LLM è°ƒç”¨
    """
    paragraphs = [p.strip() for p in book_content.split('\n') if p.strip()]
    if len(paragraphs) == 0:
        return [book_content[:max_chunk_size]] if book_content else []
    
    chunks, current_buffer, current_len = [], [], 0

    for para in paragraphs:
        para_len = len(para)
        if not current_buffer:
            current_buffer.append(para)
            current_len += para_len
            continue
        if current_len < min_chunk_size:
            current_buffer.append(para)
            current_len += para_len
            continue
        if current_len + para_len > max_chunk_size:
            chunks.append("\n".join(current_buffer))
            current_buffer = [para]
            current_len = para_len
            continue
        if is_obvious_break(para):
            chunks.append("\n".join(current_buffer))
            current_buffer = [para]
            current_len = para_len
        else:
            current_buffer.append(para)
            current_len += para_len

    if current_buffer:
        chunks.append("\n".join(current_buffer))

    return chunks


def smart_event_chunker_hybrid(
    book_content: str,
    client,
    model: str,
    min_chunk_size: int = 700,
    max_chunk_size: int = 3400,
    llm_budget: int = 35
) -> List[str]:
    """
    æ··åˆåˆ‡åˆ†ï¼šè§„åˆ™ä¸ºä¸»ï¼Œå°‘é‡ LLM æ ¡éªŒ
    - è§„åˆ™å‘½ä¸­ç›´æ¥åˆ‡åˆ†
    - ä»…åœ¨â€œä¸æ˜æ˜¾â€åœºæ™¯ä½¿ç”¨ LLMï¼Œä¸”æœ‰è°ƒç”¨ä¸Šé™
    """
    paragraphs = [p.strip() for p in book_content.split('\n') if p.strip()]
    if len(paragraphs) == 0:
        return [book_content[:max_chunk_size]] if book_content else []

    chunks, current_buffer, current_len = [], [], 0

    for para in paragraphs:
        para_len = len(para)
        if not current_buffer:
            current_buffer.append(para)
            current_len += para_len
            continue
        if current_len < min_chunk_size:
            current_buffer.append(para)
            current_len += para_len
            continue
        if current_len + para_len > max_chunk_size:
            chunks.append("\n".join(current_buffer))
            current_buffer = [para]
            current_len = para_len
            continue
        if is_obvious_break(para):
            chunks.append("\n".join(current_buffer))
            current_buffer = [para]
            current_len = para_len
            continue

        decision = "NO"
        if llm_budget > 0:
            prev_end = current_buffer[-1][-60:]
            next_start = para[:60]
            try:
                response = client.models.generate_content(
                    model=model,
                    contents=BREAKPOINT_PROMPT.format(prev_end=prev_end, next_start=next_start),
                    config=types.GenerateContentConfig(
                        max_output_tokens=3,
                        temperature=0.0
                    )
                )
                decision = response.text.strip().upper()
            except Exception:
                decision = "NO"
            llm_budget -= 1

        if "YES" in decision:
            chunks.append("\n".join(current_buffer))
            current_buffer = [para]
            current_len = para_len
        else:
            current_buffer.append(para)
            current_len += para_len

    if current_buffer:
        chunks.append("\n".join(current_buffer))

    return chunks

# ============================================
# Extraction with Structured Output + Context Injection
# ============================================
EXTRACTION_PROMPT_WITH_CONTEXT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†å²æ”¿æ²»æ–‡çŒ®åˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“ã€äº‹ä»¶å’Œå…³ç³»ã€‚

ã€å…¨å±€èƒŒæ™¯ã€‘: {global_context}
ã€å‰æƒ…æè¦ã€‘: {last_event_summary}

ã€å½“å‰æ–‡æœ¬ã€‘:
{text}

**æå–è¦æ±‚ï¼š**
1. **å®ä½“ (entities)**ï¼šæ”¿æ²»äººç‰©ã€åœ°ç‚¹ã€ç»„ç»‡ã€æ–‡ä»¶/è‘—ä½œã€æ¦‚å¿µ/ææ³•
2. **äº‹ä»¶ (events)**ï¼šä¼šè®®ã€å†²çªã€è®²è¯ã€æ”¿ç­–å‡ºå°ã€æ”¿æ²»è¿åŠ¨  
3. **å…³ç³» (relations)**ï¼šå®ä½“ä¸äº‹ä»¶ä¹‹é—´çš„æ‰€æœ‰å…³ç³»ï¼Œç”¨å…·ä½“åŠ¨è¯æè¿°

**å…³ç³»åŠ¨è¯ç¤ºä¾‹ï¼ˆå°½é‡ä½¿ç”¨å…·ä½“åŠ¨è¯ï¼‰ï¼š**
- äºº-äº‹ä»¶ï¼šå‚ä¸ã€ä¸»æŒã€å‡ºå¸­ã€å‘èµ·ã€ç»„ç»‡ã€é¢†å¯¼ã€ç­–åˆ’ã€åå¯¹ã€æ”¯æŒã€æ‰¹è¯„
- äºº-äººï¼šä»»å‘½ã€ææ‹”ã€æ‰¹è¯„ã€æ”¯æŒã€åå¯¹ã€é€®æ•ã€å¤„å†³ã€å¹³åã€ä¼šè§ã€æŒ‡ç¤º
- äºº-ç»„ç»‡ï¼šåŠ å…¥ã€é¢†å¯¼ã€åˆ›å»ºã€é€€å‡ºã€æ”¹ç»„ã€æ‹…ä»»
- äºº-åœ°ç‚¹ï¼šå‰å¾€ã€è§†å¯Ÿã€é©»å®ˆã€æ’¤ç¦»ã€æŠµè¾¾
- äº‹ä»¶-äº‹ä»¶ï¼šå¯¼è‡´ã€å¼•å‘ã€ä¿ƒæˆã€ä¸­æ–­ã€å»¶ç»­
- å…¶ä»–ï¼šç­¾ç½²ã€æ‰¹å‡†ã€æå‡ºã€å‘è¡¨ã€å®£å¸ƒã€é¢å¸ƒã€ä¿®è®¢ã€åºŸé™¤

**é£é™©ç­‰çº§åˆ¤æ–­æ ‡å‡†ï¼š**
- SAFE: ç¬¦åˆå®˜æ–¹å†å²å™äº‹
- CONTROVERSIAL: å­˜åœ¨äº‰è®®æˆ–æœªå®šè®º
- HIGH_RISK: æ˜æ˜¾è¿èƒŒå®˜æ–¹å™äº‹ã€å†å²è™šæ— ä¸»ä¹‰å€¾å‘

**IDè§„èŒƒï¼š**
- å®ä½“ID: PER_å§“å / LOC_åœ°å / ORG_ç»„ç»‡å / DOC_æ–‡ä»¶å / CON_æ¦‚å¿µå
- äº‹ä»¶ID: EVT_äº‹ä»¶ç®€ç§°_å¹´ä»½

**é‡è¦ï¼šå®å¯å¤šæå–ä¹Ÿä¸è¦é—æ¼ï¼æ¯ä¸ªå®ä½“è‡³å°‘è¦æœ‰ä¸€æ¡å…³ç³»ã€‚**

è¯·æå–æ‰€æœ‰å®ä½“ã€äº‹ä»¶å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œæ³¨æ„ä¿æŒä¸å‰æ–‡çš„IDä¸€è‡´æ€§ã€‚"""

def extract_with_context(
    client, 
    model: str, 
    text: str,
    global_context: str = "",
    last_event_summary: str = "æ— "
) -> HistoricalGraphBatch:
    """ä½¿ç”¨ Pydantic Schema è¿›è¡Œç»“æ„åŒ–æŠ½å–ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰"""
    try:
        prompt = EXTRACTION_PROMPT_WITH_CONTEXT.format(
            global_context=global_context or "å†å²æ”¿æ²»æ–‡çŒ®åˆ†æ",
            last_event_summary=last_event_summary,
            text=text
        )
        response = client.models.generate_content(
            model=model,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=HistoricalGraphBatch
            )
        )
        data = json.loads(response.text)
        return HistoricalGraphBatch(**data)
    except Exception as e:
        st.warning(f"æŠ½å–è­¦å‘Š: {e}")
        return HistoricalGraphBatch(entities=[], events=[], relations=[])


from concurrent.futures import ThreadPoolExecutor, as_completed

def process_book_pipeline(
    book_text: str,
    client,
    model: str,
    global_context: str = "",
    chunk_mode: str = "hybrid",
    llm_budget: int = 35,
    max_workers: int = 5
) -> List[HistoricalGraphBatch]:
    """
    ä¸Šä¸‹æ–‡æ³¨å…¥å‡½æ•°ï¼ˆå¹¶è¡Œä¼˜åŒ–ç‰ˆï¼‰ï¼š
    1) äº‹ä»¶åˆ‡åˆ†ï¼ˆæ··åˆ/çº¯è§„åˆ™/å›ºå®šé•¿åº¦ï¼‰
    2) å¹¶è¡ŒæŠ½å–å„å—
    3) è¿”å›æ¯å—çš„ç»“æ„åŒ–å›¾è°±
    """
    # å¤§æ–‡ä»¶ç”¨æ›´å¤§çš„ chunk å‡å°‘è°ƒç”¨æ¬¡æ•°
    text_len = len(book_text)
    if text_len > 500000:  # > 500KB
        min_size, max_size = 2000, 6000
    elif text_len > 100000:  # > 100KB
        min_size, max_size = 1200, 4500
    else:
        min_size, max_size = 700, 3400
    
    if chunk_mode == "hybrid":
        raw_chunks = smart_event_chunker_hybrid(
            book_text, client, model,
            min_chunk_size=min_size,
            max_chunk_size=max_size,
            llm_budget=llm_budget
        )
    elif chunk_mode == "fixed":
        raw_chunks = split_text_simple(book_text, size=max_size)
    else:
        raw_chunks = fast_event_chunker(book_text, min_chunk_size=min_size, max_chunk_size=max_size)

    total_chunks = len(raw_chunks)
    
    # å¹¶è¡ŒæŠ½å–ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
    all_graph_data = [None] * total_chunks
    completed = [0]  # ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
    
    progress_bar = st.progress(0, text=f"æŠ½å–è¿›åº¦: 0/{total_chunks}")
    
    def extract_chunk(idx, chunk):
        return idx, extract_with_context(
            client, model, chunk,
            global_context=global_context,
            last_event_summary="æ— "  # å¹¶è¡Œæ—¶æ— æ³•ä¸²è¡Œä¼ é€’ä¸Šä¸‹æ–‡
        )
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(extract_chunk, i, c) for i, c in enumerate(raw_chunks)]
        for future in as_completed(futures):
            idx, result = future.result()
            all_graph_data[idx] = result
            completed[0] += 1
            progress_bar.progress(
                completed[0] / total_chunks, 
                text=f"æŠ½å–è¿›åº¦: {completed[0]}/{total_chunks}"
            )
    
    progress_bar.empty()
    return [g for g in all_graph_data if g is not None]


def aggregate_graph_batches(batches: List[HistoricalGraphBatch]):
    """åˆå¹¶å¤šä¸ªæ‰¹æ¬¡çš„å›¾è°±æ•°æ®ï¼ˆå»é‡ï¼‰"""
    all_entities = {}
    all_events = {}
    all_relations = []

    for batch in batches:
        for e in batch.entities:
            if e.id not in all_entities:
                all_entities[e.id] = e.model_dump()
        for ev in batch.events:
            if ev.id not in all_events:
                all_events[ev.id] = ev.model_dump()
        for r in batch.relations:
            all_relations.append(r.model_dump())

    seen = set()
    unique_relations = []
    for r in all_relations:
        key = f"{r['source_id']}|{r['relation']}|{r['target_id']}"
        if key not in seen:
            seen.add(key)
            unique_relations.append(r)

    return list(all_entities.values()), list(all_events.values()), unique_relations


def find_orphan_nodes(entities, events, relations):
    """æ‰¾å‡ºæ²¡æœ‰ä»»ä½•å…³ç³»çš„å­¤ç«‹èŠ‚ç‚¹"""
    connected_ids = set()
    for r in relations:
        connected_ids.add(r.get('source_id', ''))
        connected_ids.add(r.get('target_id', ''))
    
    orphan_entities = [e for e in entities if e['id'] not in connected_ids]
    orphan_events = [e for e in events if e['id'] not in connected_ids]
    return orphan_entities, orphan_events


def integrate_orphans(client, model, orphan_entities, orphan_events, all_entities, all_events):
    """ä¸ºå­¤ç«‹èŠ‚ç‚¹æ¨æ–­å…³ç³»ï¼ˆåŸºäºå·²æœ‰å›¾è°±ä¸Šä¸‹æ–‡ï¼‰"""
    if not orphan_entities and not orphan_events:
        return []
    
    # æ„å»ºä¸Šä¸‹æ–‡ï¼šå·²æœ‰çš„å®ä½“å’Œäº‹ä»¶
    context_entities = [f"{e['id']}: {e['name']}" for e in all_entities[:50]]
    context_events = [f"{e['id']}: {e['name']} ({e.get('time_str', '')})" for e in all_events[:30]]
    
    orphan_list = []
    for e in orphan_entities:
        orphan_list.append(f"å®ä½“ {e['id']}: {e['name']} (ç±»å‹: {e['type']})")
    for e in orphan_events:
        orphan_list.append(f"äº‹ä»¶ {e['id']}: {e['name']} ({e.get('time_str', '')})")
    
    if not orphan_list:
        return []
    
    prompt = f"""ä»¥ä¸‹æ˜¯ä¸€ä¸ªå†å²çŸ¥è¯†å›¾è°±ä¸­çš„å­¤ç«‹èŠ‚ç‚¹ï¼ˆæ²¡æœ‰ä¸å…¶ä»–èŠ‚ç‚¹å»ºç«‹å…³ç³»ï¼‰ã€‚
è¯·æ ¹æ®ä½ å¯¹ä¸­å›½å†å²çš„äº†è§£ï¼Œä¸ºè¿™äº›å­¤ç«‹èŠ‚ç‚¹æ¨æ–­åˆç†çš„å…³ç³»ã€‚

ã€å·²æœ‰å®ä½“ã€‘:
{chr(10).join(context_entities)}

ã€å·²æœ‰äº‹ä»¶ã€‘:
{chr(10).join(context_events)}

ã€å­¤ç«‹èŠ‚ç‚¹ã€‘:
{chr(10).join(orphan_list)}

è¯·ä¸ºæ¯ä¸ªå­¤ç«‹èŠ‚ç‚¹ç”Ÿæˆ1-3æ¡ä¸å·²æœ‰èŠ‚ç‚¹çš„å…³ç³»ã€‚å…³ç³»å¿…é¡»ç¬¦åˆå†å²äº‹å®ã€‚
å¦‚æœæ— æ³•ç¡®å®šå…³ç³»ï¼Œå¯ä»¥è·³è¿‡è¯¥èŠ‚ç‚¹ã€‚

è¿”å›JSONæ ¼å¼ï¼š
{{"relations": [
  {{"source_id": "...", "target_id": "...", "relation": "åŠ¨è¯", "details": "å…·ä½“è¯´æ˜", "evidence": "åŸºäºå†å²å¸¸è¯†"}}
]}}
"""
    
    try:
        response = client.models.generate_content(
            model=model,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json"
            )
        )
        data = json.loads(response.text)
        return data.get('relations', [])
    except Exception as e:
        st.warning(f"å­¤ç«‹èŠ‚ç‚¹æ•´åˆå¤±è´¥: {e}")
        return []


# ============================================
# å…³æ³¨åº¦è¯„ä¼°ä¸å…³ç³»è¿‡æ»¤ï¼ˆäº‹ä»¶ä¸­å¿ƒï¼‰
# ============================================
FOCUS_KEYWORDS_STRONG = [
    "æ€»ä¹¦è®°", "ä¸»å¸­", "å†›å§”ä¸»å¸­", "æ€»ç†", "æ€»å¸ä»¤",
    "ä¸­å¤®å†›å§”", "å†›å§”", "æ”¿æ²»å±€", "å¸¸å§”", "ä¸­å¤®å§”å‘˜ä¼š",
    "å›½åŠ¡é™¢", "å›½å®¶å®‰å…¨", "å…¬å®‰", "å›½å®‰", "æ­¦è­¦",
    "è§£æ”¾å†›", "æˆ˜åŒº", "å†›åŒº", "æµ·å†›", "ç©ºå†›", "ç«ç®­å†›",
    "å›½é˜²éƒ¨", "å¤–äº¤éƒ¨", "ä¸­å®£éƒ¨", "ä¸­ç»„éƒ¨", "äººå¤§", "æ”¿å"
]
FOCUS_KEYWORDS_MID = [
    "å…š", "æ”¿åºœ", "å†›", "å†›é˜Ÿ", "éƒ¨é˜Ÿ", "å§”å‘˜ä¼š", "å…šå§”", "çœå§”", "å¸‚å§”",
    "æŒ‡æŒ¥éƒ¨", "å¸ä»¤éƒ¨", "æ€»å‚", "æ€»å", "æ€»æ”¿"
]

EVENT_IMPORTANCE_BASE = {
    "MEETING": 5, "POLICY": 5, "MOVEMENT": 5,
    "CONFLICT": 4, "SPEECH": 4
}

REL_BONUS = {
    "ORGANIZED": 2,
    "DEFINED_AS": 2,
    "CAUSED": 2,
    "PARTICIPATED_IN": 1
}

RISK_BONUS = {"SAFE": 1, "CONTROVERSIAL": 2, "HIGH_RISK": 3}


def _contains_any(text: str, keywords: List[str]) -> bool:
    return any(k and (k in text) for k in keywords)


def compute_entity_importance(e: dict, extra_focus: Optional[List[str]] = None) -> int:
    name = e.get("name", "")
    etype = e.get("type", "")
    base = {"PERSON": 5, "ORG": 4, "DOCUMENT": 3, "CONCEPT": 3, "LOCATION": 2}.get(etype, 2)
    score = base

    if _contains_any(name, FOCUS_KEYWORDS_STRONG):
        score += 4
    if _contains_any(name, FOCUS_KEYWORDS_MID):
        score += 2
    if extra_focus and _contains_any(name, extra_focus):
        score += 3

    return min(score, 10)


def compute_event_importance(ev: dict, extra_focus: Optional[List[str]] = None) -> int:
    base = EVENT_IMPORTANCE_BASE.get(ev.get("type", ""), 3)
    text = f"{ev.get('name','')} {ev.get('political_significance','')}"
    score = base

    if _contains_any(text, FOCUS_KEYWORDS_STRONG):
        score += 3
    if _contains_any(text, FOCUS_KEYWORDS_MID):
        score += 1
    if extra_focus and _contains_any(text, extra_focus):
        score += 2

    score += RISK_BONUS.get(ev.get("risk_level", "SAFE"), 1)
    return min(score, 10)


def prioritize_graph(entities, events, relations, min_weight=5, top_per_event=8, extra_focus: Optional[List[str]] = None):
    entity_score = {e["id"]: compute_entity_importance(e, extra_focus=extra_focus) for e in entities}
    event_score = {ev["id"]: compute_event_importance(ev, extra_focus=extra_focus) for ev in events}

    filtered = []
    event_relations = defaultdict(list)

    for r in relations:
        src, tgt = r.get("source_id"), r.get("target_id")
        if src in event_score or tgt in event_score:
            src_score = event_score.get(src, entity_score.get(src, 2))
            tgt_score = event_score.get(tgt, entity_score.get(tgt, 2))
            rel_bonus = REL_BONUS.get(r.get("relation"), 0)
            weight = min(int((src_score + tgt_score) / 2 + rel_bonus), 10)
            r["weight"] = weight
            filtered.append(r)
            event_id = src if src in event_score else tgt
            event_relations[event_id].append(r)

    keep = set()
    for event_id, rels in event_relations.items():
        rels_sorted = sorted(rels, key=lambda x: x.get("weight", 0), reverse=True)
        for r in rels_sorted[:top_per_event]:
            keep.add(f"{r['source_id']}|{r['relation']}|{r['target_id']}")
        for r in rels_sorted:
            if r.get("weight", 0) >= min_weight:
                keep.add(f"{r['source_id']}|{r['relation']}|{r['target_id']}")

    final_relations = []
    for r in filtered:
        key = f"{r['source_id']}|{r['relation']}|{r['target_id']}"
        if key in keep:
            final_relations.append(r)

    focus_nodes = sum(1 for s in list(entity_score.values()) + list(event_score.values()) if s >= 7)
    focus_relations = sum(1 for r in final_relations if r.get("weight", 0) >= 7)

    return entities, events, final_relations, {"nodes": focus_nodes, "relations": focus_relations}


# é«˜å±å…³é”®è¯ï¼ˆä¸å½’å…¥æ•£ç‚¹å®¹å™¨ï¼‰
HIGH_RISK_KEYWORDS = [
    "æ¸…æ´—", "è‚ƒå", "æ•´é£", "æ‰¹æ–—", "è¿«å®³", "å†¤æ¡ˆ", "å¹³å", "å¤„å†³", "æªå†³",
    "ä¸‘é—»", "è…è´¥", "è´ªæ±¡", "å—è´¿", "åŒè§„", "è½é©¬", "è°ƒæŸ¥", "å®¡æŸ¥",
    "æ”¿å˜", "å…µå˜", "å›é€ƒ", "æš—æ€", "é‡åˆº", "è‡ªæ€", "éæ­£å¸¸æ­»äº¡",
    "å…­å››", "å¤©å®‰é—¨", "åå³", "æ–‡é©", "å¤§è·ƒè¿›", "ä¸‰å¹´å›°éš¾",
    "æ—å½ª", "å››äººå¸®", "æ±Ÿé’", "åº·ç”Ÿ", "å‘¨æ°¸åº·", "è–„ç†™æ¥", "ä»¤è®¡åˆ’", "å¾æ‰åš", "éƒ­ä¼¯é›„"
]


def is_high_risk_node(node):
    """åˆ¤æ–­èŠ‚ç‚¹æ˜¯å¦æ¶‰åŠé«˜å±å†…å®¹"""
    # æ£€æŸ¥äº‹ä»¶é£é™©ç­‰çº§
    if node.get("risk_level") in ("HIGH_RISK", "CONTROVERSIAL"):
        return True
    
    # æ£€æŸ¥åç§°å’Œæè¿°æ˜¯å¦åŒ…å«é«˜å±å…³é”®è¯
    text = f"{node.get('name', '')} {node.get('description', '')} {node.get('political_significance', '')}"
    for kw in HIGH_RISK_KEYWORDS:
        if kw in text:
            return True
    return False


def find_sparse_nodes(entities, events, relations, max_relations=2):
    """æ‰¾å‡ºå…³ç³»ç¨€ç–çš„èŠ‚ç‚¹"""
    node_relation_count = defaultdict(int)
    for r in relations:
        node_relation_count[r.get("source_id", "")] += 1
        node_relation_count[r.get("target_id", "")] += 1
    
    sparse_entities = [e for e in entities if node_relation_count.get(e["id"], 0) <= max_relations]
    sparse_events = [ev for ev in events if node_relation_count.get(ev["id"], 0) <= max_relations]
    main_entities = [e for e in entities if node_relation_count.get(e["id"], 0) > max_relations]
    main_events = [ev for ev in events if node_relation_count.get(ev["id"], 0) > max_relations]
    
    return main_entities, main_events, sparse_entities, sparse_events


def integrate_sparse_with_search(client, model, sparse_entities, sparse_events, main_entities, main_events):
    """
    ç”¨ LLM + å¤–éƒ¨çŸ¥è¯†ä¸ºæ•£ç‚¹æ‰¾ä¸»å›¾å…³è”
    è¿”å›ï¼šæ–°å…³ç³»åˆ—è¡¨ï¼Œæœªæ•´åˆçš„èŠ‚ç‚¹åˆ—è¡¨
    """
    if not sparse_entities and not sparse_events:
        return [], [], []
    
    # æ„å»ºä¸»å›¾ä¸Šä¸‹æ–‡
    main_entity_names = [f"{e['id']}: {e['name']}" for e in main_entities[:40]]
    main_event_names = [f"{ev['id']}: {ev['name']} ({ev.get('time_str','')})" for ev in main_events[:30]]
    
    # æ•£ç‚¹åˆ—è¡¨
    sparse_list = []
    for e in sparse_entities:
        sparse_list.append({"id": e["id"], "name": e["name"], "type": e.get("type", ""), "is_event": False})
    for ev in sparse_events:
        sparse_list.append({"id": ev["id"], "name": ev["name"], "type": ev.get("type", ""), "time": ev.get("time_str", ""), "is_event": True})
    
    if not sparse_list:
        return [], [], []
    
    prompt = f"""ä½ æ˜¯ä¸­å›½è¿‘ç°ä»£å²ä¸“å®¶ã€‚ä»¥ä¸‹æ•£ç‚¹èŠ‚ç‚¹åœ¨çŸ¥è¯†å›¾è°±ä¸­å…³ç³»ç¨€ç–ï¼Œè¯·æ ¹æ®å†å²äº‹å®ä¸ºå®ƒä»¬æ‰¾åˆ°ä¸ä¸»å›¾èŠ‚ç‚¹çš„å…³è”ã€‚

ã€ä¸»å›¾å®ä½“ã€‘:
{chr(10).join(main_entity_names)}

ã€ä¸»å›¾äº‹ä»¶ã€‘:
{chr(10).join(main_event_names)}

ã€æ•£ç‚¹èŠ‚ç‚¹ã€‘:
{json.dumps(sparse_list, ensure_ascii=False, indent=2)}

**ä»»åŠ¡ï¼š**
1. æ ¹æ®ä½ å¯¹ä¸­å›½å†å²çš„äº†è§£ï¼Œä¸ºæ¯ä¸ªæ•£ç‚¹æ‰¾1-3æ¡ä¸ä¸»å›¾èŠ‚ç‚¹çš„çœŸå®å…³ç³»
2. å…³ç³»å¿…é¡»ç¬¦åˆå†å²äº‹å®ï¼Œä¸èƒ½ç¼–é€ 
3. å¦‚æœæŸä¸ªæ•£ç‚¹ç¡®å®ä¸ä¸»å›¾æ— å…³ï¼Œå°†å…¶æ ‡è®°ä¸º unlinked

**è¿”å›JSONæ ¼å¼ï¼š**
{{
  "new_relations": [
    {{"source_id": "æ•£ç‚¹ID", "target_id": "ä¸»å›¾èŠ‚ç‚¹ID", "relation": "åŠ¨è¯", "details": "è¯´æ˜", "evidence": "å†å²ä¾æ®"}}
  ],
  "unlinked_ids": ["æ— æ³•å…³è”çš„æ•£ç‚¹IDåˆ—è¡¨"]
}}
"""
    
    try:
        response = client.models.generate_content(
            model=model,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json"
            )
        )
        data = json.loads(response.text)
        new_relations = data.get("new_relations", [])
        unlinked_ids = set(data.get("unlinked_ids", []))
        
        # åˆ†ç¦»å·²æ•´åˆå’Œæœªæ•´åˆçš„æ•£ç‚¹
        unlinked_entities = [e for e in sparse_entities if e["id"] in unlinked_ids]
        unlinked_events = [ev for ev in sparse_events if ev["id"] in unlinked_ids]
        
        return new_relations, unlinked_entities, unlinked_events
    except Exception as e:
        st.warning(f"æ•£ç‚¹æ•´åˆæ£€ç´¢å¤±è´¥: {e}")
        return [], sparse_entities, sparse_events


def build_sensitive_node(unlinked_entities, unlinked_events):
    """å°†æ— æ³•å…³è”çš„æ•£ç‚¹æ„å»ºä¸ºæ•æ„Ÿè¯åº“èŠ‚ç‚¹"""
    if not unlinked_entities and not unlinked_events:
        return None
    
    items = []
    for e in unlinked_entities:
        items.append(f"â€¢ {e['name']} ({ENTITY_TYPE_CN.get(e.get('type'), e.get('type'))})")
    for ev in unlinked_events:
        items.append(f"â—† {ev['name']} ({ev.get('time_str', '')})")
    
    return {
        "id": "_SENSITIVE_TERMS_",
        "label": f"æ•æ„Ÿè¯åº“ ({len(unlinked_entities) + len(unlinked_events)})",
        "items": items,
        "entities": unlinked_entities,
        "events": unlinked_events
    }


# ============================================
# Graph Building - Event-Centric
# ============================================
def build_event_graph(entities, events, relations, sensitive_node=None):
    """æ„å»ºä»¥äº‹ä»¶ä¸ºä¸­å¿ƒçš„å›¾è°±"""
    G = nx.DiGraph()
    
    # é¢œè‰²é…ç½® - å…šå²æ–‡çŒ®é£æ ¼
    entity_colors = {
        "PERSON": "#c62828", "LOCATION": "#1565c0", "ORG": "#2e7d32",
        "DOCUMENT": "#7b1fa2", "CONCEPT": "#f57c00"
    }
    
    event_colors = {
        "MEETING": "#1565c0", "CONFLICT": "#c62828", "SPEECH": "#2e7d32",
        "POLICY": "#7b1fa2", "MOVEMENT": "#f57c00"
    }
    
    risk_border = {
        "SAFE": "#2e7d32", "CONTROVERSIAL": "#f57c00", "HIGH_RISK": "#c62828"
    }
    
    # æ·»åŠ å®ä½“èŠ‚ç‚¹
    entity_map = {e["id"]: e for e in entities}
    for e in entities:
        G.add_node(
            e["id"],
            label=e["name"],
            color=entity_colors.get(e["type"], "#94a3b8"),
            size=20,
            shape="dot",
            title=f"{ENTITY_TYPE_CN.get(e['type'], e['type'])}: {e['name']}"
        )
    
    # æ·»åŠ äº‹ä»¶èŠ‚ç‚¹ï¼ˆæ›´å¤§ã€æ–¹å½¢è¡¨ç¤ºï¼‰
    event_map = {ev["id"]: ev for ev in events}
    for ev in events:
        risk = ev.get("risk_level", "SAFE")
        bg = event_colors.get(ev.get("type"), "#64748b")
        border = risk_border.get(risk, "#d1d5db")
        G.add_node(
            ev["id"],
            label=ev["name"],
            color={"background": bg, "border": border},
            size=38,
            shape="diamond",
            borderWidth=3,
            borderWidthSelected=5,
            title=(
                f"ã€{EVENT_TYPE_CN.get(ev.get('type'), ev.get('type'))}ã€‘{ev.get('name','')}\n"
                f"æ—¶é—´: {ev.get('time_str', 'æœªçŸ¥')}\n"
                f"é£é™©: {RISK_LEVEL_CN.get(risk, risk)}\n\n"
                f"{ev.get('description', '')}"
            )
        )
    
    # æ·»åŠ æ•æ„Ÿè¯åº“èŠ‚ç‚¹
    if sensitive_node:
        items = sensitive_node.get("items", [])
        G.add_node(
            sensitive_node["id"],
            label=sensitive_node["label"],
            color={"background": "#fef3c7", "border": "#f59e0b"},
            size=50,
            shape="box",
            borderWidth=3,
            font={"color": "#92400e", "size": 14, "bold": True},
            title="âš ï¸ æ•æ„Ÿè¯åº“ï¼ˆæ— æ³•å…³è”åˆ°ä¸»å›¾ï¼‰:\n\n" + "\n".join(items[:50]) + ("\n..." if len(items) > 50 else "")
        )
    
    # æ·»åŠ å…³ç³»è¾¹ï¼ˆä½¿ç”¨ç²¾é€‰æƒé‡ï¼Œå‡å°‘å†—ä½™ï¼‰
    for r in relations:
        src, tgt = r["source_id"], r["target_id"]
        if (src in entity_map or src in event_map) and (tgt in entity_map or tgt in event_map):
            w = int(r.get("weight", 5))
            edge_color = "#C41E3A" if w >= 7 else ("#666666" if w >= 5 else "#aaaaaa")
            G.add_edge(
                src, tgt,
                label=r.get("relation", ""),
                title=(r.get("details", "") + ("\n\nè¯æ®: " + r.get("evidence", "") if r.get("evidence") else "")),
                color=edge_color,
                width=1 + w / 3
            )
    
    return G

# API Config
with st.container():
    col1, col2, col3, col4 = st.columns([1, 2, 2, 1])
    with col2:
        api_key = st.text_input("API Key", type="password", placeholder="Gemini API Key", label_visibility="collapsed")
    with col3:
        model = st.text_input("Model", value="gemini-3-flash-preview", placeholder="æ¨¡å‹åç§°", label_visibility="collapsed")

st.markdown("<br>", unsafe_allow_html=True)

# Step Navigation
def can_go(target):
    if target == 1:
        return True
    elif target == 2:
        return len(st.session_state.events) > 0
    elif target == 3:
        return len(st.session_state.relations) > 0
    return False

col1, col2, col3, col4, col5 = st.columns([2, 1, 1, 1, 2])

with col2:
    icon = "âœ“" if st.session_state.step > 1 else ("â—" if st.session_state.step == 1 else "â—‹")
    if st.button(f"{icon} ä¸Šä¼ ", key="nav1", use_container_width=True):
        st.session_state.step = 1
        st.rerun()

with col3:
    icon = "âœ“" if st.session_state.step > 2 else ("â—" if st.session_state.step == 2 else "â—‹")
    if st.button(f"{icon} å®¡æ ¸", key="nav2", use_container_width=True, disabled=not can_go(2)):
        st.session_state.step = 2
        st.rerun()

with col4:
    icon = "âœ“" if st.session_state.step > 3 else ("â—" if st.session_state.step == 3 else "â—‹")
    if st.button(f"{icon} å›¾è°±", key="nav3", use_container_width=True, disabled=not can_go(3)):
        st.session_state.step = 3
        st.rerun()

st.markdown("<hr style='border:none; border-top:1px solid #e5e5e5; margin:20px 0;'>", unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.markdown("### ğŸ”§ å·¥å…·")
    if st.button("ğŸ”„ é‡æ–°å¼€å§‹", use_container_width=True):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()
    
    st.markdown("---")
    st.markdown("**ç»Ÿè®¡**")
    st.write(f"å®ä½“: {len(st.session_state.entities)}")
    st.write(f"äº‹ä»¶: {len(st.session_state.events)}")
    st.write(f"å…³ç³»: {len(st.session_state.relations)}")

# ============================================
# Step 1: Upload & Extract
# ============================================
if st.session_state.step == 1:
    st.markdown("<br>", unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        # å…¨å±€èƒŒæ™¯è¾“å…¥
        global_context = st.text_area(
            "å…¨å±€èƒŒæ™¯ï¼ˆå¯é€‰ï¼‰",
            placeholder="ç®€è¦æè¿°æ–‡æ¡£ä¸»é¢˜ï¼Œå¦‚ï¼šæœ¬ä¹¦è®²è¿°ä¸­å›½å…±äº§å…š1921-1949å¹´å‘å±•å†ç¨‹...",
            height=68,
            help="æä¾›èƒŒæ™¯æœ‰åŠ©äº LLM æ›´å‡†ç¡®åœ°æŠ½å–å†…å®¹"
        )
        
        # åˆ‡åˆ†æ¨¡å¼
        chunk_mode = st.radio(
            "åˆ‡åˆ†æ¨¡å¼",
            ["æ··åˆåˆ‡åˆ†ï¼ˆæ¨èï¼‰", "çº¯è§„åˆ™", "å›ºå®šé•¿åº¦"],
            horizontal=True,
            help="æ··åˆåˆ‡åˆ†ï¼šè§„åˆ™+å°‘é‡ LLM æ ¡éªŒï¼Œå…¼é¡¾å‡†ç¡®ä¸é€Ÿåº¦"
        )
        
        # Recall ä¼˜å…ˆï¼šé»˜è®¤ç»™æ›´é«˜çš„ LLM æ ¡éªŒé¢„ç®—ï¼Œé™ä½è¯¯åˆ‡å¯¼è‡´çš„æ¼æŠ½
        llm_budget = 35
        if "æ··åˆ" in chunk_mode:
            llm_budget = st.slider(
                "LLM æ–­ç‚¹æ ¡éªŒä¸Šé™",
                min_value=0,
                max_value=120,
                value=35,
                step=5,
                help="Recall ä¼˜å…ˆï¼šå€¼è¶Šå¤§è¶Šä¸å®¹æ˜“æ¼äº‹ä»¶ï¼ˆä½†ä¼šå˜æ…¢ï¼‰"
            )

        st.markdown("---")
        with st.expander("Advanced Settings", expanded=False):
            focus_extra_raw = st.text_input(
                "é¢å¤–å…³æ³¨å…³é”®è¯ï¼ˆå¯é€‰ï¼‰",
                placeholder="ç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼šå…šæ”¿å†›,æŸæœºæ„,æŸèŒåŠ¡,æŸéƒ¨é˜Ÿ...",
                help="ä¼šæå‡ä½ å…³å¿ƒçš„èŠ‚ç‚¹/äº‹ä»¶æƒé‡ï¼Œå¸®åŠ©ä¿ç•™æ›´å¤šç›¸å…³å…³ç³»"
            )
            min_weight = st.slider(
                "ä¿ç•™å…³ç³»æœ€ä½æƒé‡",
                min_value=1,
                max_value=10,
                value=3,
                step=1,
                help="Recall ä¼˜å…ˆå»ºè®® 2-4ï¼šè¶Šä½è¶Šä¸æ¼ï¼ˆä½†æ›´å†—ä½™ï¼‰"
            )
            top_per_event = st.slider(
                "æ¯ä¸ªäº‹ä»¶è‡³å°‘ä¿ç•™å‰ N æ¡å…³ç³»",
                min_value=3,
                max_value=30,
                value=12,
                step=1,
                help="Recall ä¼˜å…ˆå»ºè®® 10-15ï¼šä¿è¯æ¯ä¸ªäº‹ä»¶ä¸è¢«å‰ªç˜¦"
            )
        
        files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", accept_multiple_files=True, type=["pdf", "epub", "docx", "txt"],
                                 label_visibility="collapsed")
        
        if files:
            st.markdown(f"<p style='text-align:center; color:#86868b;'>å·²é€‰æ‹© {len(files)} ä¸ªæ–‡ä»¶</p>", 
                       unsafe_allow_html=True)
            
            if st.button("å¼€å§‹åˆ†æ", use_container_width=True):
                if not api_key:
                    st.error("è¯·å¡«å†™ API Key")
                else:
                    all_text = ""
                    for f in files:
                        all_text += read_file(f, api_key) + "\n\n"
                    
                    if len(all_text.strip()) < 100:
                        st.error("æ–‡ä»¶å†…å®¹è¿‡å°‘")
                    else:
                        st.session_state.text_content = all_text
                        st.session_state.global_context = global_context
                        client = get_client(api_key)
                        
                        mode = "hybrid" if "æ··åˆ" in chunk_mode else ("rule" if "è§„åˆ™" in chunk_mode else "fixed")
                        
                        # ä¸Šä¸‹æ–‡æ³¨å…¥æŠ½å–ï¼ˆåˆ†å—å¤„ç†ï¼‰
                        with st.spinner("æ­£åœ¨è¿›è¡Œä¸Šä¸‹æ–‡æ³¨å…¥æŠ½å–..."):
                            batches = process_book_pipeline(
                                all_text, client, model,
                                global_context=global_context,
                                chunk_mode=mode,
                                llm_budget=llm_budget
                            )
                            entities, events, relations = aggregate_graph_batches(batches)
                            
                            # æ•´åˆå­¤ç«‹èŠ‚ç‚¹
                            orphan_entities, orphan_events = find_orphan_nodes(entities, events, relations)
                            if orphan_entities or orphan_events:
                                st.info(f"ğŸ”— æ­£åœ¨æ•´åˆ {len(orphan_entities)} ä¸ªå­¤ç«‹å®ä½“, {len(orphan_events)} ä¸ªå­¤ç«‹äº‹ä»¶...")
                                extra_relations = integrate_orphans(
                                    client, model, 
                                    orphan_entities, orphan_events,
                                    entities, events
                                )
                                relations.extend(extra_relations)

                        extra_focus = [s.strip() for s in (focus_extra_raw or "").split(",") if s.strip()]

                        # å…³ç³»å»å†—ä½™ï¼šä»¥äº‹ä»¶ä¸ºä¸­å¿ƒï¼Œç¡®ä¿æ¯ä¸ªäº‹ä»¶æœ‰ top-Nï¼ŒåŒæ—¶æŒ‰æƒé‡è¿‡æ»¤
                        entities, events, relations, stats = prioritize_graph(
                            entities, events, relations,
                            min_weight=min_weight,
                            top_per_event=top_per_event,
                            extra_focus=extra_focus
                        )
                        st.session_state.focus_stats = stats

                        st.info(f"ğŸ“„ åˆ‡åˆ†å®Œæˆ: {len(batches)} å— Â· ç²¾é€‰å {len(relations)} æ¡å…³ç³»")
                        
                        if events:
                            st.session_state.entities = entities
                            st.session_state.events = events
                            st.session_state.relations = relations
                            st.success(f"âœ… å®Œæˆ: {len(entities)} å®ä½“, {len(events)} äº‹ä»¶, {len(relations)} å…³ç³»")
                            st.session_state.step = 2
                            st.rerun()
                        else:
                            st.error("æœªè¯†åˆ«åˆ°äº‹ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹")

# ============================================
# Step 2: Review Events & Entities
# ============================================
elif st.session_state.step == 2:
    st.markdown("""
    <div style="text-align:center; padding:30px 0 40px; background: linear-gradient(180deg, rgba(196,30,58,0.05) 0%, #fff 100%);">
        <div class="hero-badge" style="margin-bottom:16px;">å†…å®¹å®¡æ ¸</div>
        <h1 style="font-size:32px; color:#C41E3A; letter-spacing:0.1em;">å®¡æ ¸ä¸è°ƒæ•´</h1>
        <p style="color:#7a7a7a; font-size:14px;">æŸ¥çœ‹æŠ½å–çš„äº‹ä»¶å’Œå®ä½“ï¼Œè°ƒæ•´é£é™©ç­‰çº§</p>
    </div>
    """, unsafe_allow_html=True)
    
    entities = st.session_state.entities
    events = st.session_state.events
    relations = st.session_state.relations
    
    # Stats
    focus_stats = st.session_state.focus_stats or {"nodes": 0, "relations": 0}
    col1, col2, col3, col4, col5, col6 = st.columns(6)
    with col1:
        st.metric("å®ä½“æ•°", len(entities))
    with col2:
        st.metric("äº‹ä»¶æ•°", len(events))
    with col3:
        st.metric("å…³ç³»æ•°", len(relations))
    with col4:
        high_risk = sum(1 for e in events if e.get("risk_level") == "HIGH_RISK")
        st.metric("é«˜é£é™©äº‹ä»¶", high_risk)
    with col5:
        st.metric("é‡ç‚¹èŠ‚ç‚¹", focus_stats.get("nodes", 0))
    with col6:
        st.metric("é‡ç‚¹å…³ç³»", focus_stats.get("relations", 0))
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    # Tabs
    tab1, tab2, tab3 = st.tabs(["ğŸ“… äº‹ä»¶", "ğŸ‘¤ å®ä½“", "ğŸ”— å…³ç³»"])
    
    with tab1:
        st.markdown("#### äº‹ä»¶åˆ—è¡¨ï¼ˆæŒ‰é£é™©æ’åºï¼‰")
        
        # æŒ‰é£é™©ç­‰çº§æ’åº
        risk_order = {"HIGH_RISK": 0, "CONTROVERSIAL": 1, "SAFE": 2}
        sorted_events = sorted(events, key=lambda x: risk_order.get(x.get("risk_level", "SAFE"), 2))
        
        for ev in sorted_events:
            risk = ev.get("risk_level", "SAFE")
            risk_class = f"risk-{risk.lower().replace('_', '-')}"
            type_class = f"event-{ev.get('type', 'MEETING').lower()}"
            
            st.markdown(f"""
            <div class="event-card">
                <div style="display:flex; justify-content:space-between; align-items:center;">
                    <span class="event-title">{ev.get('name', 'æœªçŸ¥äº‹ä»¶')}</span>
                    <span class="risk-tag {risk_class}">{RISK_LEVEL_CN.get(risk, risk)}</span>
                </div>
                <div class="event-meta">
                    <span class="type-tag {type_class}">{EVENT_TYPE_CN.get(ev.get('type'), ev.get('type'))}</span>
                    ğŸ“… {ev.get('time_str', 'æ—¶é—´æœªçŸ¥')}
                </div>
                <div class="event-desc">{ev.get('description', '')}</div>
                <div style="margin-top:10px; padding-top:10px; border-top:1px solid #f0f0f0;">
                    <strong style="font-size:13px; color:#6e6e73;">æ”¿æ²»å®šæ€§:</strong>
                    <span style="font-size:13px; color:#1d1d1f;">{ev.get('political_significance', 'æœªå®šæ€§')}</span>
                </div>
            </div>
            """, unsafe_allow_html=True)
    
    with tab2:
        st.markdown("#### å®ä½“åˆ—è¡¨")
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type = defaultdict(list)
        for e in entities:
            by_type[e.get("type", "OTHER")].append(e)
        
        for etype in ["PERSON", "ORG", "LOCATION", "DOCUMENT", "CONCEPT"]:
            if etype not in by_type:
                continue
            elist = by_type[etype]
            type_class = f"type-{etype.lower()}"
            
            with st.expander(f"**{ENTITY_TYPE_CN.get(etype, etype)}** Â· {len(elist)} ä¸ª", expanded=(etype == "PERSON")):
                for e in elist:
                    alias_str = f" ({', '.join(e.get('alias', []))})" if e.get('alias') else ""
                    st.markdown(f"""
                    <div style="padding:8px 12px; margin:4px 0; background:#f5f5f7; border-radius:8px;">
                        <span class="type-tag {type_class}">{etype}</span>
                        <strong>{e.get('name', '')}</strong>
                        <span style="color:#86868b;">{alias_str}</span>
                    </div>
                    """, unsafe_allow_html=True)
    
    with tab3:
        st.markdown("#### å…³ç³»åˆ—è¡¨")
        
        for r in relations[:50]:
            rel_cn = RELATION_TYPE_CN.get(r.get("relation"), r.get("relation"))
            st.markdown(f"""
            <div style="padding:10px 14px; margin:6px 0; background:#f5f5f7; border-radius:8px;">
                <strong>{r.get('source_id', '')}</strong>
                <span style="color:#0071e3; margin:0 8px;">â€”[ {rel_cn} ]â†’</span>
                <strong>{r.get('target_id', '')}</strong>
                <div style="font-size:12px; color:#86868b; margin-top:4px;">{r.get('details', '')}</div>
            </div>
            """, unsafe_allow_html=True)
        
        if len(relations) > 50:
            st.markdown(f"<p style='color:#86868b;'>... è¿˜æœ‰ {len(relations)-50} æ¡å…³ç³»</p>", unsafe_allow_html=True)
    
    # Navigation
    st.markdown("<br>", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        c1, c2 = st.columns(2)
        with c1:
            if st.button("â† é‡æ–°ä¸Šä¼ ", use_container_width=True):
                st.session_state.step = 1
                st.rerun()
        with c2:
            if st.button("ç”Ÿæˆå›¾è°± â†’", use_container_width=True, type="primary"):
                st.session_state.step = 3
                st.rerun()

# ============================================
# Step 3: Graph Visualization
# ============================================
elif st.session_state.step == 3:
    st.markdown("""
    <div style="text-align:center; padding:30px 0 30px; background: linear-gradient(180deg, rgba(196,30,58,0.05) 0%, #fff 100%);">
        <div class="hero-badge" style="margin-bottom:16px;">å¯è§†åŒ–å±•ç¤º</div>
        <h1 style="font-size:32px; color:#C41E3A; letter-spacing:0.1em;">äº‹ä»¶çŸ¥è¯†å›¾è°±</h1>
        <p style="color:#7a7a7a; font-size:14px;">â—† è±å½¢ä¸ºäº‹ä»¶èŠ‚ç‚¹ &nbsp;|&nbsp; â— åœ†å½¢ä¸ºå®ä½“èŠ‚ç‚¹</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Filters
    col1, col2, col3, col4 = st.columns([2, 2, 1, 1])
    with col1:
        show_types = st.multiselect(
            "æ˜¾ç¤ºäº‹ä»¶ç±»å‹",
            options=list(EVENT_TYPE_CN.keys()),
            default=list(EVENT_TYPE_CN.keys()),
            format_func=lambda x: EVENT_TYPE_CN.get(x, x)
        )
    with col2:
        show_risks = st.multiselect(
            "æ˜¾ç¤ºé£é™©ç­‰çº§",
            options=list(RISK_LEVEL_CN.keys()),
            default=list(RISK_LEVEL_CN.keys()),
            format_func=lambda x: RISK_LEVEL_CN.get(x, x)
        )
    with col3:
        layout = st.selectbox("å¸ƒå±€", ["åŠ›å¯¼å‘", "å±‚æ¬¡å¸ƒå±€"])
    with col4:
        sparse_threshold = st.number_input("æ•£ç‚¹è¡¥é“¾é˜ˆå€¼", min_value=1, max_value=5, value=2, help="å…³ç³»æ•°â‰¤æ­¤å€¼ä¼šå°è¯•è¡¥é“¾ï¼Œè¡¥ä¸ä¸Šåˆ™è¿›å…¥æ•æ„Ÿè¯åº“")
    
    # è¿‡æ»¤äº‹ä»¶
    filtered_events = [
        e for e in st.session_state.events
        if e.get("type") in show_types and e.get("risk_level", "SAFE") in show_risks
    ]
    
    # è¿‡æ»¤å…³ç³»ï¼ˆåªä¿ç•™ä¸è¿‡æ»¤åäº‹ä»¶ç›¸å…³çš„ï¼‰
    event_ids = {e["id"] for e in filtered_events}
    filtered_relations = [
        r for r in st.session_state.relations
        if r["source_id"] in event_ids or r["target_id"] in event_ids
    ]
    
    # è¿‡æ»¤å®ä½“ï¼ˆåªä¿ç•™æœ‰å…³ç³»çš„ï¼‰
    involved_ids = set()
    for r in filtered_relations:
        involved_ids.add(r["source_id"])
        involved_ids.add(r["target_id"])
    
    filtered_entities = [e for e in st.session_state.entities if e["id"] in involved_ids]
    
    # è¯†åˆ«æ•£ç‚¹ï¼ˆå…³ç³»ç¨€ç–çš„èŠ‚ç‚¹ï¼‰
    main_entities, main_events, sparse_entities, sparse_events = find_sparse_nodes(
        filtered_entities, filtered_events, filtered_relations, max_relations=sparse_threshold
    )
    
    # æ„å»ºæ•æ„Ÿè¯åº“èŠ‚ç‚¹ï¼ˆæ•£ç‚¹å½’å…¥æ­¤å¤„ï¼‰
    sensitive_node = build_sensitive_node(sparse_entities, sparse_events)
    
    # ä¸»å›¾åªä¿ç•™å…³ç³»å¯†é›†çš„èŠ‚ç‚¹
    main_ids = set([e["id"] for e in main_entities] + [ev["id"] for ev in main_events])
    main_relations = [
        r for r in filtered_relations
        if r.get("source_id") in main_ids and r.get("target_id") in main_ids
    ]
    
    sensitive_count = len(sparse_entities) + len(sparse_events)
    st.markdown(
        f"<p style='text-align:center; color:#86868b;'>ä¸»å›¾: {len(main_events)} äº‹ä»¶, {len(main_entities)} å®ä½“, {len(main_relations)} å…³ç³»"
        + (f" Â· <b>æ•æ„Ÿè¯åº“: {sensitive_count} é¡¹</b>" if sensitive_count > 0 else "") + "</p>", 
        unsafe_allow_html=True
    )
    
    # Build & Display Graph
    G = build_event_graph(main_entities, main_events, main_relations, sensitive_node)
    
    net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="#1d1d1f", directed=True)
    net.from_nx(G)
    
    if layout == "å±‚æ¬¡å¸ƒå±€":
        net.set_options('''
        {
          "layout": {"hierarchical": {"enabled": true, "direction": "UD", "sortMethod": "directed"}},
          "physics": {"enabled": false},
          "edges": {"smooth": {"type": "cubicBezier"}, "font": {"size": 11}},
          "interaction": {"hover": true, "navigationButtons": true}
        }
        ''')
    else:
        net.set_options('''
        {
          "physics": {
            "solver": "forceAtlas2Based",
            "forceAtlas2Based": {"gravitationalConstant": -100, "springLength": 150},
            "stabilization": {"iterations": 200}
          },
          "edges": {"smooth": {"type": "continuous"}, "font": {"size": 11}},
          "interaction": {"hover": true, "navigationButtons": true}
        }
        ''')
    
    st.components.v1.html(net.generate_html(), height=620)
    
    # Legend
    st.markdown("""
    <div style="display:flex; justify-content:center; gap:20px; margin-top:16px; flex-wrap:wrap; padding:12px 20px; background:#fafafa; border:1px solid #e5e5e5;">
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#1565c0;">â—†</span> ä¼šè®®</span>
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#c62828;">â—†</span> å†²çª</span>
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#2e7d32;">â—†</span> è®²è¯</span>
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#7b1fa2;">â—†</span> æ”¿ç­–</span>
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#f57c00;">â—†</span> è¿åŠ¨</span>
        <span style="color:#ccc;">|</span>
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#c62828;">â—</span> äººç‰©</span>
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#2e7d32;">â—</span> ç»„ç»‡</span>
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#1565c0;">â—</span> åœ°ç‚¹</span>
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#7b1fa2;">â—</span> æ–‡ä»¶</span>
        <span style="font-size:12px; color:#4a4a4a;"><span style="color:#f57c00;">â—</span> æ¦‚å¿µ</span>
    </div>
    """, unsafe_allow_html=True)
    
    # æ•æ„Ÿè¯åº“è¯¦æƒ…å±•å¼€
    if sensitive_node:
        sensitive_count = len(sensitive_node.get("items", []))
        with st.expander(f"âš ï¸ æ•æ„Ÿè¯åº“è¯¦æƒ… ({sensitive_count} é¡¹)", expanded=False):
            items = sensitive_node.get("items", [])
            st.markdown("\n".join(items[:200]) + ("\n..." if len(items) > 200 else ""))
    
    # Export
    st.markdown("<br>", unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        export_data = {
            "entities": st.session_state.entities,
            "events": st.session_state.events,
            "relations": st.session_state.relations
        }
        c1, c2 = st.columns(2)
        with c1:
            st.download_button(
                "ğŸ“¥ ä¸‹è½½ JSON",
                json.dumps(export_data, ensure_ascii=False, indent=2),
                "event_graph.json",
                use_container_width=True
            )
        with c2:
            if st.button("â† è¿”å›å®¡æ ¸", use_container_width=True):
                st.session_state.step = 2
                st.rerun()
